{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f1e75a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T08:56:31.324277Z",
     "start_time": "2023-10-07T08:56:29.630144Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fe4098",
   "metadata": {},
   "source": [
    "## 还有一个类对于autograd的实现非常重要：Function。Tensor 和 Function 互相连接生成了一个无环图 (acyclic graph)，它编码了完整的计算历史。每个张量都有一个.grad_fn属性，该属性引用了创建 Tensor 自身的Function(除非这个张量是用户手动创建的，即这个张量的grad_fn是 None )。下面给出的例子中，张量由用户手动创建，因此grad_fn返回结果是None。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2bcc6df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T09:07:36.279453Z",
     "start_time": "2023-10-07T09:07:36.266297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "x = torch.randn(3,3,requires_grad=True)\n",
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2ff72d",
   "metadata": {},
   "source": [
    "## 如果需要计算导数，可以在 Tensor 上调用 .backward()。如果 Tensor 是一个标量(即它包含一个元素的数据），则不需要为 backward() 指定任何参数，但是如果它有更多的元素，则需要指定一个gradient参数，该参数是形状匹配的张量。\n",
    "## 创建一个张量并设置requires_grad=True用来追踪其计算历史"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3216c762",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T09:07:43.185301Z",
     "start_time": "2023-10-07T09:07:43.170261Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x =torch.ones(2,2,requires_grad = True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0a58e7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T09:00:12.969232Z",
     "start_time": "2023-10-07T09:00:12.920972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 对这个张量做一次运算：\n",
    "y = x**2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caf27b83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T09:00:39.528859Z",
     "start_time": "2023-10-07T09:00:39.513884Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PowBackward0 object at 0x00000188761A6BF0>\n"
     ]
    }
   ],
   "source": [
    "# y是计算的结果，所以它有grad_fn属性。\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a5eb7ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T09:01:34.235014Z",
     "start_time": "2023-10-07T09:01:34.222599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<MulBackward0>) tensor(3., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 对 y 进行更多操作\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be54118",
   "metadata": {},
   "source": [
    "## .requires_grad_(...) 原地改变了现有张量的requires_grad标志。如果没有指定的话，默认输入的这个标志是 False。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38ca3ca1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T09:02:24.882551Z",
     "start_time": "2023-10-07T09:02:24.845429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x000001885F264B50>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2) # 缺失情况下默认 requires_grad = False\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39031774",
   "metadata": {},
   "source": [
    "# 梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c184d52e",
   "metadata": {},
   "source": [
    "## 现在开始进行反向传播，因为 out 是一个标量，因此out.backward()和 out.backward(torch.tensor(1.)) 等价。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74acaa2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T09:03:42.229233Z",
     "start_time": "2023-10-07T09:03:42.180689Z"
    }
   },
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fb1e4d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T09:04:23.341884Z",
     "start_time": "2023-10-07T09:04:23.335905Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c5d5ab6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T09:07:57.011030Z",
     "start_time": "2023-10-07T09:07:56.995603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 输出导数 d(out)/dx\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1433870",
   "metadata": {},
   "source": [
    "## 注意：grad在反向传播过程中是累加的(accumulated)，这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以一般在反向传播之前需把梯度清零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90d61aee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T09:09:32.234328Z",
     "start_time": "2023-10-07T09:09:32.221240Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.])\n",
      "tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# 再来反向传播⼀一次，注意grad是累加的\n",
    "out2 = x.sum()\n",
    "out2.backward()\n",
    "print(x.grad)\n",
    "\n",
    "out3 = x.sum()\n",
    "x.grad.data.zero_()\n",
    "out3.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "358571e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T09:09:32.469692Z",
     "start_time": "2023-10-07T09:09:32.461268Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4411,  2.0444,  0.3594], requires_grad=True)\n",
      "tensor([-225.8194, 1046.7155,  184.0074], grad_fn=<MulBackward0>)\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# 现在我们来看一个雅可比向量积的例子：\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y = x * 2\n",
    "i = 0\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "    i = i + 1\n",
    "print(y)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8850f12c",
   "metadata": {},
   "source": [
    "## 在这种情况下，y 不再是标量。torch.autograd 不能直接计算完整的雅可比矩阵，但是如果我们只想要雅可比向量积，只需将这个向量作为参数传给 backward："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9936d524",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T09:10:15.965511Z",
     "start_time": "2023-10-07T09:10:15.907759Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(v)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03042be8",
   "metadata": {},
   "source": [
    "## 也可以通过将代码块包装在 with torch.no_grad(): 中，来阻止 autograd 跟踪设置了.requires_grad=True的张量的历史记录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46243ce8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T09:10:40.713685Z",
     "start_time": "2023-10-07T09:10:40.705713Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1d4977",
   "metadata": {},
   "source": [
    "## 如果我们想要修改 tensor 的数值，但是又不希望被 autograd 记录(即不会影响反向传播)， 那么我们可以对 tensor.data 进行操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19009e23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T09:11:25.788162Z",
     "start_time": "2023-10-07T09:11:25.773611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n",
      "False\n",
      "tensor([100.], requires_grad=True)\n",
      "tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(1,requires_grad=True)\n",
    "\n",
    "print(x.data) # 还是一个tensor\n",
    "print(x.data.requires_grad) # 但是已经是独立于计算图之外\n",
    "\n",
    "y = 2 * x\n",
    "x.data *= 100 # 只改变了值，不会记录在计算图，所以不会影响梯度传播\n",
    "\n",
    "y.backward()\n",
    "print(x) # 更改data的值也会影响tensor的值 \n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bcef29e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T09:46:05.539563Z",
     "start_time": "2023-10-07T09:46:05.525604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# 例子\n",
    "x=torch.tensor(1.,requires_grad=True)\n",
    "b=torch.tensor(2.,requires_grad=True)\n",
    "w=torch.tensor(3.,requires_grad=True)\n",
    "z=w*x+b\n",
    "z.backward()\n",
    "#反向传播\n",
    "print(x.grad)#x导数值\n",
    "print(w.grad)#w导数值\n",
    "print(b.grad)#b导数值\n",
    "# 注：要想使上面的x,b,w支持求导，必须让它们为浮点类型，也就是我们给初始值的时候要加个点：“.”。不然的话，就会报错。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c6bdeb0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T09:48:40.700207Z",
     "start_time": "2023-10-07T09:48:40.682330Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12., grad_fn=<CopyBackwards>)\n",
      "tensor(6., grad_fn=<CopyBackwards>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.).requires_grad_()\n",
    "y = torch.tensor(3.).requires_grad_()\n",
    "z = x * x * y\n",
    "z.backward(create_graph=True) # x.grad = 12\n",
    "print(x.grad)\n",
    "x.grad.data.zero_() #PyTorch使用backward()时默认会累加梯度，需要手动把前一次的梯度清零\n",
    "x.grad.backward() #对x一次求导后为2xy，然后再次反向传播\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0c2be9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
