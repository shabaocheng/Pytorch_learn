{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a11e249",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T06:27:15.348647Z",
     "start_time": "2023-10-11T06:27:15.330942Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56049650",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T06:27:16.328260Z",
     "start_time": "2023-10-11T06:27:16.319505Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module torch.nn.init in torch.nn:\n",
      "\n",
      "NAME\n",
      "    torch.nn.init\n",
      "\n",
      "FUNCTIONS\n",
      "    calculate_gain(nonlinearity, param=None)\n",
      "        Return the recommended gain value for the given nonlinearity function.\n",
      "        The values are as follows:\n",
      "        \n",
      "        ================= ====================================================\n",
      "        nonlinearity      gain\n",
      "        ================= ====================================================\n",
      "        Linear / Identity :math:`1`\n",
      "        Conv{1,2,3}D      :math:`1`\n",
      "        Sigmoid           :math:`1`\n",
      "        Tanh              :math:`\\frac{5}{3}`\n",
      "        ReLU              :math:`\\sqrt{2}`\n",
      "        Leaky Relu        :math:`\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}`\n",
      "        SELU              :math:`\\frac{3}{4}`\n",
      "        ================= ====================================================\n",
      "        \n",
      "        .. warning::\n",
      "            In order to implement `Self-Normalizing Neural Networks`_ ,\n",
      "            you should use ``nonlinearity='linear'`` instead of ``nonlinearity='selu'``.\n",
      "            This gives the initial weights a variance of ``1 / N``,\n",
      "            which is necessary to induce a stable fixed point in the forward pass.\n",
      "            In contrast, the default gain for ``SELU`` sacrifices the normalisation\n",
      "            effect for more stable gradient flow in rectangular layers.\n",
      "        \n",
      "        Args:\n",
      "            nonlinearity: the non-linear function (`nn.functional` name)\n",
      "            param: optional parameter for the non-linear function\n",
      "        \n",
      "        Examples:\n",
      "            >>> gain = nn.init.calculate_gain('leaky_relu', 0.2)  # leaky_relu with negative_slope=0.2\n",
      "        \n",
      "        .. _Self-Normalizing Neural Networks: https://papers.nips.cc/paper/2017/hash/5d44ee6f2c3f71b73125876103c8f6c4-Abstract.html\n",
      "    \n",
      "    constant(*args, **kwargs)\n",
      "        constant(...)\n",
      "        \n",
      "        .. warning::\n",
      "            This method is now deprecated in favor of :func:`torch.nn.init.constant_`.\n",
      "        \n",
      "        See :func:`~torch.nn.init.constant_` for details.\n",
      "    \n",
      "    constant_(tensor: torch.Tensor, val: float) -> torch.Tensor\n",
      "        Fills the input Tensor with the value :math:`\\text{val}`.\n",
      "        \n",
      "        Args:\n",
      "            tensor: an n-dimensional `torch.Tensor`\n",
      "            val: the value to fill the tensor with\n",
      "        \n",
      "        Examples:\n",
      "            >>> w = torch.empty(3, 5)\n",
      "            >>> nn.init.constant_(w, 0.3)\n",
      "    \n",
      "    dirac(*args, **kwargs)\n",
      "        dirac(...)\n",
      "        \n",
      "        .. warning::\n",
      "            This method is now deprecated in favor of :func:`torch.nn.init.dirac_`.\n",
      "        \n",
      "        See :func:`~torch.nn.init.dirac_` for details.\n",
      "    \n",
      "    dirac_(tensor, groups=1)\n",
      "        Fills the {3, 4, 5}-dimensional input `Tensor` with the Dirac\n",
      "        delta function. Preserves the identity of the inputs in `Convolutional`\n",
      "        layers, where as many input channels are preserved as possible. In case\n",
      "        of groups>1, each group of channels preserves identity\n",
      "        \n",
      "        Args:\n",
      "            tensor: a {3, 4, 5}-dimensional `torch.Tensor`\n",
      "            groups (int, optional): number of groups in the conv layer (default: 1)\n",
      "        Examples:\n",
      "            >>> w = torch.empty(3, 16, 5, 5)\n",
      "            >>> nn.init.dirac_(w)\n",
      "            >>> w = torch.empty(3, 24, 5, 5)\n",
      "            >>> nn.init.dirac_(w, 3)\n",
      "    \n",
      "    eye(*args, **kwargs)\n",
      "        eye(...)\n",
      "        \n",
      "        .. warning::\n",
      "            This method is now deprecated in favor of :func:`torch.nn.init.eye_`.\n",
      "        \n",
      "        See :func:`~torch.nn.init.eye_` for details.\n",
      "    \n",
      "    eye_(tensor)\n",
      "        Fills the 2-dimensional input `Tensor` with the identity\n",
      "        matrix. Preserves the identity of the inputs in `Linear` layers, where as\n",
      "        many inputs are preserved as possible.\n",
      "        \n",
      "        Args:\n",
      "            tensor: a 2-dimensional `torch.Tensor`\n",
      "        \n",
      "        Examples:\n",
      "            >>> w = torch.empty(3, 5)\n",
      "            >>> nn.init.eye_(w)\n",
      "    \n",
      "    kaiming_normal(*args, **kwargs)\n",
      "        kaiming_normal(...)\n",
      "        \n",
      "        .. warning::\n",
      "            This method is now deprecated in favor of :func:`torch.nn.init.kaiming_normal_`.\n",
      "        \n",
      "        See :func:`~torch.nn.init.kaiming_normal_` for details.\n",
      "    \n",
      "    kaiming_normal_(tensor: torch.Tensor, a: float = 0, mode: str = 'fan_in', nonlinearity: str = 'leaky_relu')\n",
      "        Fills the input `Tensor` with values according to the method\n",
      "        described in `Delving deep into rectifiers: Surpassing human-level\n",
      "        performance on ImageNet classification` - He, K. et al. (2015), using a\n",
      "        normal distribution. The resulting tensor will have values sampled from\n",
      "        :math:`\\mathcal{N}(0, \\text{std}^2)` where\n",
      "        \n",
      "        .. math::\n",
      "            \\text{std} = \\frac{\\text{gain}}{\\sqrt{\\text{fan\\_mode}}}\n",
      "        \n",
      "        Also known as He initialization.\n",
      "        \n",
      "        Args:\n",
      "            tensor: an n-dimensional `torch.Tensor`\n",
      "            a: the negative slope of the rectifier used after this layer (only\n",
      "                used with ``'leaky_relu'``)\n",
      "            mode: either ``'fan_in'`` (default) or ``'fan_out'``. Choosing ``'fan_in'``\n",
      "                preserves the magnitude of the variance of the weights in the\n",
      "                forward pass. Choosing ``'fan_out'`` preserves the magnitudes in the\n",
      "                backwards pass.\n",
      "            nonlinearity: the non-linear function (`nn.functional` name),\n",
      "                recommended to use only with ``'relu'`` or ``'leaky_relu'`` (default).\n",
      "        \n",
      "        Examples:\n",
      "            >>> w = torch.empty(3, 5)\n",
      "            >>> nn.init.kaiming_normal_(w, mode='fan_out', nonlinearity='relu')\n",
      "    \n",
      "    kaiming_uniform(*args, **kwargs)\n",
      "        kaiming_uniform(...)\n",
      "        \n",
      "        .. warning::\n",
      "            This method is now deprecated in favor of :func:`torch.nn.init.kaiming_uniform_`.\n",
      "        \n",
      "        See :func:`~torch.nn.init.kaiming_uniform_` for details.\n",
      "    \n",
      "    kaiming_uniform_(tensor: torch.Tensor, a: float = 0, mode: str = 'fan_in', nonlinearity: str = 'leaky_relu')\n",
      "        Fills the input `Tensor` with values according to the method\n",
      "        described in `Delving deep into rectifiers: Surpassing human-level\n",
      "        performance on ImageNet classification` - He, K. et al. (2015), using a\n",
      "        uniform distribution. The resulting tensor will have values sampled from\n",
      "        :math:`\\mathcal{U}(-\\text{bound}, \\text{bound})` where\n",
      "        \n",
      "        .. math::\n",
      "            \\text{bound} = \\text{gain} \\times \\sqrt{\\frac{3}{\\text{fan\\_mode}}}\n",
      "        \n",
      "        Also known as He initialization.\n",
      "        \n",
      "        Args:\n",
      "            tensor: an n-dimensional `torch.Tensor`\n",
      "            a: the negative slope of the rectifier used after this layer (only\n",
      "                used with ``'leaky_relu'``)\n",
      "            mode: either ``'fan_in'`` (default) or ``'fan_out'``. Choosing ``'fan_in'``\n",
      "                preserves the magnitude of the variance of the weights in the\n",
      "                forward pass. Choosing ``'fan_out'`` preserves the magnitudes in the\n",
      "                backwards pass.\n",
      "            nonlinearity: the non-linear function (`nn.functional` name),\n",
      "                recommended to use only with ``'relu'`` or ``'leaky_relu'`` (default).\n",
      "        \n",
      "        Examples:\n",
      "            >>> w = torch.empty(3, 5)\n",
      "            >>> nn.init.kaiming_uniform_(w, mode='fan_in', nonlinearity='relu')\n",
      "    \n",
      "    normal(*args, **kwargs)\n",
      "        normal(...)\n",
      "        \n",
      "        .. warning::\n",
      "            This method is now deprecated in favor of :func:`torch.nn.init.normal_`.\n",
      "        \n",
      "        See :func:`~torch.nn.init.normal_` for details.\n",
      "    \n",
      "    normal_(tensor: torch.Tensor, mean: float = 0.0, std: float = 1.0) -> torch.Tensor\n",
      "        Fills the input Tensor with values drawn from the normal\n",
      "        distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`.\n",
      "        \n",
      "        Args:\n",
      "            tensor: an n-dimensional `torch.Tensor`\n",
      "            mean: the mean of the normal distribution\n",
      "            std: the standard deviation of the normal distribution\n",
      "        \n",
      "        Examples:\n",
      "            >>> w = torch.empty(3, 5)\n",
      "            >>> nn.init.normal_(w)\n",
      "    \n",
      "    ones_(tensor: torch.Tensor) -> torch.Tensor\n",
      "        Fills the input Tensor with the scalar value `1`.\n",
      "        \n",
      "        Args:\n",
      "            tensor: an n-dimensional `torch.Tensor`\n",
      "        \n",
      "        Examples:\n",
      "            >>> w = torch.empty(3, 5)\n",
      "            >>> nn.init.ones_(w)\n",
      "    \n",
      "    orthogonal(*args, **kwargs)\n",
      "        orthogonal(...)\n",
      "        \n",
      "        .. warning::\n",
      "            This method is now deprecated in favor of :func:`torch.nn.init.orthogonal_`.\n",
      "        \n",
      "        See :func:`~torch.nn.init.orthogonal_` for details.\n",
      "    \n",
      "    orthogonal_(tensor, gain=1)\n",
      "        Fills the input `Tensor` with a (semi) orthogonal matrix, as\n",
      "        described in `Exact solutions to the nonlinear dynamics of learning in deep\n",
      "        linear neural networks` - Saxe, A. et al. (2013). The input tensor must have\n",
      "        at least 2 dimensions, and for tensors with more than 2 dimensions the\n",
      "        trailing dimensions are flattened.\n",
      "        \n",
      "        Args:\n",
      "            tensor: an n-dimensional `torch.Tensor`, where :math:`n \\geq 2`\n",
      "            gain: optional scaling factor\n",
      "        \n",
      "        Examples:\n",
      "            >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_LAPACK)\n",
      "            >>> w = torch.empty(3, 5)\n",
      "            >>> nn.init.orthogonal_(w)\n",
      "    \n",
      "    sparse(*args, **kwargs)\n",
      "        sparse(...)\n",
      "        \n",
      "        .. warning::\n",
      "            This method is now deprecated in favor of :func:`torch.nn.init.sparse_`.\n",
      "        \n",
      "        See :func:`~torch.nn.init.sparse_` for details.\n",
      "    \n",
      "    sparse_(tensor, sparsity, std=0.01)\n",
      "        Fills the 2D input `Tensor` as a sparse matrix, where the\n",
      "        non-zero elements will be drawn from the normal distribution\n",
      "        :math:`\\mathcal{N}(0, 0.01)`, as described in `Deep learning via\n",
      "        Hessian-free optimization` - Martens, J. (2010).\n",
      "        \n",
      "        Args:\n",
      "            tensor: an n-dimensional `torch.Tensor`\n",
      "            sparsity: The fraction of elements in each column to be set to zero\n",
      "            std: the standard deviation of the normal distribution used to generate\n",
      "                the non-zero values\n",
      "        \n",
      "        Examples:\n",
      "            >>> w = torch.empty(3, 5)\n",
      "            >>> nn.init.sparse_(w, sparsity=0.1)\n",
      "    \n",
      "    trunc_normal_(tensor: torch.Tensor, mean: float = 0.0, std: float = 1.0, a: float = -2.0, b: float = 2.0) -> torch.Tensor\n",
      "        Fills the input Tensor with values drawn from a truncated\n",
      "        normal distribution. The values are effectively drawn from the\n",
      "        normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n",
      "        with values outside :math:`[a, b]` redrawn until they are within\n",
      "        the bounds. The method used for generating the random values works\n",
      "        best when :math:`a \\leq \\text{mean} \\leq b`.\n",
      "        \n",
      "        Args:\n",
      "            tensor: an n-dimensional `torch.Tensor`\n",
      "            mean: the mean of the normal distribution\n",
      "            std: the standard deviation of the normal distribution\n",
      "            a: the minimum cutoff value\n",
      "            b: the maximum cutoff value\n",
      "        \n",
      "        Examples:\n",
      "            >>> w = torch.empty(3, 5)\n",
      "            >>> nn.init.trunc_normal_(w)\n",
      "    \n",
      "    uniform(*args, **kwargs)\n",
      "        uniform(...)\n",
      "        \n",
      "        .. warning::\n",
      "            This method is now deprecated in favor of :func:`torch.nn.init.uniform_`.\n",
      "        \n",
      "        See :func:`~torch.nn.init.uniform_` for details.\n",
      "    \n",
      "    uniform_(tensor: torch.Tensor, a: float = 0.0, b: float = 1.0) -> torch.Tensor\n",
      "        Fills the input Tensor with values drawn from the uniform\n",
      "        distribution :math:`\\mathcal{U}(a, b)`.\n",
      "        \n",
      "        Args:\n",
      "            tensor: an n-dimensional `torch.Tensor`\n",
      "            a: the lower bound of the uniform distribution\n",
      "            b: the upper bound of the uniform distribution\n",
      "        \n",
      "        Examples:\n",
      "            >>> w = torch.empty(3, 5)\n",
      "            >>> nn.init.uniform_(w)\n",
      "    \n",
      "    xavier_normal(*args, **kwargs)\n",
      "        xavier_normal(...)\n",
      "        \n",
      "        .. warning::\n",
      "            This method is now deprecated in favor of :func:`torch.nn.init.xavier_normal_`.\n",
      "        \n",
      "        See :func:`~torch.nn.init.xavier_normal_` for details.\n",
      "    \n",
      "    xavier_normal_(tensor: torch.Tensor, gain: float = 1.0) -> torch.Tensor\n",
      "        Fills the input `Tensor` with values according to the method\n",
      "        described in `Understanding the difficulty of training deep feedforward\n",
      "        neural networks` - Glorot, X. & Bengio, Y. (2010), using a normal\n",
      "        distribution. The resulting tensor will have values sampled from\n",
      "        :math:`\\mathcal{N}(0, \\text{std}^2)` where\n",
      "        \n",
      "        .. math::\n",
      "            \\text{std} = \\text{gain} \\times \\sqrt{\\frac{2}{\\text{fan\\_in} + \\text{fan\\_out}}}\n",
      "        \n",
      "        Also known as Glorot initialization.\n",
      "        \n",
      "        Args:\n",
      "            tensor: an n-dimensional `torch.Tensor`\n",
      "            gain: an optional scaling factor\n",
      "        \n",
      "        Examples:\n",
      "            >>> w = torch.empty(3, 5)\n",
      "            >>> nn.init.xavier_normal_(w)\n",
      "    \n",
      "    xavier_uniform(*args, **kwargs)\n",
      "        xavier_uniform(...)\n",
      "        \n",
      "        .. warning::\n",
      "            This method is now deprecated in favor of :func:`torch.nn.init.xavier_uniform_`.\n",
      "        \n",
      "        See :func:`~torch.nn.init.xavier_uniform_` for details.\n",
      "    \n",
      "    xavier_uniform_(tensor: torch.Tensor, gain: float = 1.0) -> torch.Tensor\n",
      "        Fills the input `Tensor` with values according to the method\n",
      "        described in `Understanding the difficulty of training deep feedforward\n",
      "        neural networks` - Glorot, X. & Bengio, Y. (2010), using a uniform\n",
      "        distribution. The resulting tensor will have values sampled from\n",
      "        :math:`\\mathcal{U}(-a, a)` where\n",
      "        \n",
      "        .. math::\n",
      "            a = \\text{gain} \\times \\sqrt{\\frac{6}{\\text{fan\\_in} + \\text{fan\\_out}}}\n",
      "        \n",
      "        Also known as Glorot initialization.\n",
      "        \n",
      "        Args:\n",
      "            tensor: an n-dimensional `torch.Tensor`\n",
      "            gain: an optional scaling factor\n",
      "        \n",
      "        Examples:\n",
      "            >>> w = torch.empty(3, 5)\n",
      "            >>> nn.init.xavier_uniform_(w, gain=nn.init.calculate_gain('relu'))\n",
      "    \n",
      "    zeros_(tensor: torch.Tensor) -> torch.Tensor\n",
      "        Fills the input Tensor with the scalar value `0`.\n",
      "        \n",
      "        Args:\n",
      "            tensor: an n-dimensional `torch.Tensor`\n",
      "        \n",
      "        Examples:\n",
      "            >>> w = torch.empty(3, 5)\n",
      "            >>> nn.init.zeros_(w)\n",
      "\n",
      "FILE\n",
      "    d:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\init.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.nn.init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41086227",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T06:27:16.695752Z",
     "start_time": "2023-10-11T06:27:16.675981Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tensor',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_calculate_correct_fan',\n",
       " '_calculate_fan_in_and_fan_out',\n",
       " '_make_deprecate',\n",
       " '_no_grad_fill_',\n",
       " '_no_grad_normal_',\n",
       " '_no_grad_trunc_normal_',\n",
       " '_no_grad_uniform_',\n",
       " '_no_grad_zero_',\n",
       " 'calculate_gain',\n",
       " 'constant',\n",
       " 'constant_',\n",
       " 'dirac',\n",
       " 'dirac_',\n",
       " 'eye',\n",
       " 'eye_',\n",
       " 'kaiming_normal',\n",
       " 'kaiming_normal_',\n",
       " 'kaiming_uniform',\n",
       " 'kaiming_uniform_',\n",
       " 'math',\n",
       " 'normal',\n",
       " 'normal_',\n",
       " 'ones_',\n",
       " 'orthogonal',\n",
       " 'orthogonal_',\n",
       " 'sparse',\n",
       " 'sparse_',\n",
       " 'torch',\n",
       " 'trunc_normal_',\n",
       " 'uniform',\n",
       " 'uniform_',\n",
       " 'warnings',\n",
       " 'xavier_normal',\n",
       " 'xavier_normal_',\n",
       " 'xavier_uniform',\n",
       " 'xavier_uniform_',\n",
       " 'zeros_']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(torch.nn.init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff7c1b1",
   "metadata": {},
   "source": [
    "#### 这些函数除了calculate_gain，所有函数的后缀都带有下划线，意味着这些函数将会直接原地更改输入张量的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48304cd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T06:27:36.645753Z",
     "start_time": "2023-10-11T06:27:36.637794Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "conv = nn.Conv2d(1,3,3)\n",
    "linear = nn.Linear(10,1)\n",
    "\n",
    "\n",
    "# 根据实际模型来使用torch.nn.init进行初始化，通常使用isinstance()来进行判断模块\n",
    "\n",
    "print(isinstance(conv,nn.Conv2d)) # 判断conv是否是nn.Conv2d类型\n",
    "print(isinstance(linear,nn.Conv2d)) # 判断linear是否是nn.Conv2d类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b85b638f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T06:28:20.656996Z",
     "start_time": "2023-10-11T06:28:20.649024Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.3289, -0.2766, -0.1190],\n",
       "          [ 0.0156,  0.0821, -0.1018],\n",
       "          [-0.2432, -0.1440, -0.1805]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1478, -0.3053, -0.0254],\n",
       "          [ 0.1240, -0.0659, -0.2035],\n",
       "          [-0.3191,  0.2859,  0.2953]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1710,  0.3329, -0.0473],\n",
       "          [ 0.1467, -0.1095,  0.0724],\n",
       "          [-0.0642, -0.0382,  0.0065]]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看随机初始化的conv参数\n",
    "conv.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25acec90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T06:28:29.140548Z",
     "start_time": "2023-10-11T06:28:29.131578Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1689,  0.0414, -0.0216, -0.2563,  0.1744, -0.3080, -0.2602, -0.0347,\n",
       "         -0.0663, -0.0339]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看linear的参数\n",
    "linear.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecb17e15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T06:29:45.281439Z",
     "start_time": "2023-10-11T06:29:45.273699Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0702, -0.2754, -0.3377],\n",
       "          [ 0.0186, -0.0928,  0.1448],\n",
       "          [ 0.0885, -0.4235, -0.0674]]],\n",
       "\n",
       "\n",
       "        [[[ 0.3098,  0.0483,  0.1748],\n",
       "          [ 0.0786, -0.0998, -0.0588],\n",
       "          [-0.2620,  0.1402, -0.2494]]],\n",
       "\n",
       "\n",
       "        [[[-0.0374,  0.1345,  0.0779],\n",
       "          [ 0.1921, -0.1648, -0.1613],\n",
       "          [ 0.1003,  0.2392,  0.2052]]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xavier\n",
    "torch.nn.init.xavier_normal_(conv.weight.data)\n",
    "conv.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba9ea307",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T06:29:59.853818Z",
     "start_time": "2023-10-11T06:29:59.842901Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000,\n",
       "         0.3000]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对linear进行常数初始化\n",
    "torch.nn.init.constant_(linear.weight.data,0.3)\n",
    "linear.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "551eca4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T06:30:19.890497Z",
     "start_time": "2023-10-11T06:30:19.875445Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.2191e-01, -1.1305e-01, -4.6943e-01],\n",
       "          [-4.9135e-01, -2.2668e-01, -8.9071e-03],\n",
       "          [-3.4801e-02,  5.0942e-01,  1.7515e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.8005e-01,  1.5025e-02,  4.0557e-01],\n",
       "          [-4.9781e-02,  2.7651e-01, -1.5404e-01],\n",
       "          [-3.4019e-01, -1.7847e-01,  1.5932e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 4.1476e-01, -1.0942e+00,  5.3046e-01],\n",
       "          [-5.5255e-01,  4.5096e-04,  3.4765e-01],\n",
       "          [-1.8132e-02,  1.1355e+00,  2.6482e-01]]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对conv进行kaiming初始化\n",
    "torch.nn.init.kaiming_normal_(conv.weight.data)\n",
    "conv.weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bc46a9",
   "metadata": {},
   "source": [
    "### 初始化函数的封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e622cdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T06:36:38.749423Z",
     "start_time": "2023-10-11T06:36:38.736208Z"
    }
   },
   "outputs": [],
   "source": [
    "# 人们常常将各种初始化方法定义为一个initialize_weights()的函数并在模型初始后进行使用。\n",
    "def initialize_weights(model):\n",
    "\tfor m in model.modules():\n",
    "\t\t# 判断是否属于Conv2d\n",
    "\t\tif isinstance(m, nn.Conv2d):\n",
    "\t\t\ttorch.nn.init.zeros_(m.weight.data)\n",
    "\t\t\t# 判断是否有偏置\n",
    "\t\t\tif m.bias is not None:\n",
    "\t\t\t\ttorch.nn.init.constant_(m.bias.data,0.3)\n",
    "\t\telif isinstance(m, nn.Linear):\n",
    "\t\t\ttorch.nn.init.normal_(m.weight.data, 0.1)\n",
    "\t\t\tif m.bias is not None:\n",
    "\t\t\t\ttorch.nn.init.zeros_(m.bias.data)\n",
    "\t\telif isinstance(m, nn.BatchNorm2d):\n",
    "\t\t\tm.weight.data.fill_(1) \t\t \n",
    "\t\t\tm.bias.data.zeros_()\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb6465f",
   "metadata": {},
   "source": [
    "### 这段代码流程是遍历当前模型的每一层，然后判断各层属于什么类型，然后根据不同类型层，设定不同的权值初始化方法。我们可以通过下面的例程进行一个简短的演示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa64d511",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T06:36:40.690702Z",
     "start_time": "2023-10-11T06:36:40.678023Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.2791, -0.1291, -0.1524],\n",
      "          [-0.2811,  0.3133,  0.1566],\n",
      "          [ 0.1856, -0.3264,  0.1985]]]])\n",
      "-------初始化-------\n",
      "tensor([[[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]]])\n"
     ]
    }
   ],
   "source": [
    "# 模型的定义\n",
    "class MLP(nn.Module):\n",
    "  # 声明带有模型参数的层，这里声明了两个全连接层\n",
    "  def __init__(self, **kwargs):\n",
    "    # 调用MLP父类Block的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数\n",
    "    super(MLP, self).__init__(**kwargs)\n",
    "    self.hidden = nn.Conv2d(1,1,3)\n",
    "    self.act = nn.ReLU()\n",
    "    self.output = nn.Linear(10,1)\n",
    "    \n",
    "   # 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出\n",
    "  def forward(self, x):\n",
    "    o = self.act(self.hidden(x))\n",
    "    return self.output(o)\n",
    "\n",
    "mlp = MLP()\n",
    "print(mlp.hidden.weight.data)\n",
    "print(\"-------初始化-------\")\n",
    "\n",
    "mlp.apply(initialize_weights)\n",
    "# 或者initialize_weights(mlp)\n",
    "print(mlp.hidden.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a4115f",
   "metadata": {},
   "source": [
    "## 注意： 我们在初始化时，最好不要将模型的参数初始化为0，因为这样会导致梯度消失，从而影响模型的训练效果。因此，我们在初始化时，可以使用其他初始化方法或者将模型初始化为一个很小的值，如0.01，0.1等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c17483a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
