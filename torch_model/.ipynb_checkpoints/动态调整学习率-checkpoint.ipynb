{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a072184d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T09:14:17.553022Z",
     "start_time": "2023-10-16T09:14:17.549148Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da9cd472",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T09:14:34.025016Z",
     "start_time": "2023-10-16T09:14:33.923361Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module torch.optim.lr_scheduler in torch.optim:\n",
      "\n",
      "NAME\n",
      "    torch.optim.lr_scheduler\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        LRScheduler\n",
      "            ChainedScheduler\n",
      "            ConstantLR\n",
      "            CosineAnnealingLR\n",
      "            CosineAnnealingWarmRestarts\n",
      "            CyclicLR\n",
      "            ExponentialLR\n",
      "            LambdaLR\n",
      "            LinearLR\n",
      "            MultiStepLR\n",
      "            MultiplicativeLR\n",
      "            OneCycleLR\n",
      "            PolynomialLR\n",
      "            SequentialLR\n",
      "            StepLR\n",
      "        ReduceLROnPlateau\n",
      "    \n",
      "    class ChainedScheduler(LRScheduler)\n",
      "     |  ChainedScheduler(schedulers)\n",
      "     |  \n",
      "     |  Chains list of learning rate schedulers. It takes a list of chainable learning\n",
      "     |  rate schedulers and performs consecutive step() functions belonging to them by just\n",
      "     |  one call.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      schedulers (list): List of chained schedulers.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      >>> # xdoctest: +SKIP\n",
      "     |      >>> # Assuming optimizer uses lr = 1. for all groups\n",
      "     |      >>> # lr = 0.09     if epoch == 0\n",
      "     |      >>> # lr = 0.081    if epoch == 1\n",
      "     |      >>> # lr = 0.729    if epoch == 2\n",
      "     |      >>> # lr = 0.6561   if epoch == 3\n",
      "     |      >>> # lr = 0.59049  if epoch >= 4\n",
      "     |      >>> scheduler1 = ConstantLR(self.opt, factor=0.1, total_iters=2)\n",
      "     |      >>> scheduler2 = ExponentialLR(self.opt, gamma=0.9)\n",
      "     |      >>> scheduler = ChainedScheduler([scheduler1, scheduler2])\n",
      "     |      >>> for epoch in range(100):\n",
      "     |      >>>     train(...)\n",
      "     |      >>>     validate(...)\n",
      "     |      >>>     scheduler.step()\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ChainedScheduler\n",
      "     |      LRScheduler\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, schedulers)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict)\n",
      "     |      Loads the schedulers state.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): scheduler state. Should be an object returned\n",
      "     |              from a call to :meth:`state_dict`.\n",
      "     |  \n",
      "     |  state_dict(self)\n",
      "     |      Returns the state of the scheduler as a :class:`dict`.\n",
      "     |      \n",
      "     |      It contains an entry for every variable in self.__dict__ which\n",
      "     |      is not the optimizer.\n",
      "     |      The wrapped scheduler states will also be saved.\n",
      "     |  \n",
      "     |  step(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LRScheduler:\n",
      "     |  \n",
      "     |  get_last_lr(self)\n",
      "     |      Return last computed learning rate by current scheduler.\n",
      "     |  \n",
      "     |  get_lr(self)\n",
      "     |  \n",
      "     |  print_lr(self, is_verbose, group, lr, epoch=None)\n",
      "     |      Display the current learning rate.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LRScheduler:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class ConstantLR(LRScheduler)\n",
      "     |  ConstantLR(optimizer, factor=0.3333333333333333, total_iters=5, last_epoch=-1, verbose=False)\n",
      "     |  \n",
      "     |  Decays the learning rate of each parameter group by a small constant factor until the\n",
      "     |  number of epoch reaches a pre-defined milestone: total_iters. Notice that such decay can\n",
      "     |  happen simultaneously with other changes to the learning rate from outside this scheduler.\n",
      "     |  When last_epoch=-1, sets initial lr as lr.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      optimizer (Optimizer): Wrapped optimizer.\n",
      "     |      factor (float): The number we multiply learning rate until the milestone. Default: 1./3.\n",
      "     |      total_iters (int): The number of steps that the scheduler decays the learning rate.\n",
      "     |          Default: 5.\n",
      "     |      last_epoch (int): The index of the last epoch. Default: -1.\n",
      "     |      verbose (bool): If ``True``, prints a message to stdout for\n",
      "     |          each update. Default: ``False``.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      >>> # xdoctest: +SKIP\n",
      "     |      >>> # Assuming optimizer uses lr = 0.05 for all groups\n",
      "     |      >>> # lr = 0.025   if epoch == 0\n",
      "     |      >>> # lr = 0.025   if epoch == 1\n",
      "     |      >>> # lr = 0.025   if epoch == 2\n",
      "     |      >>> # lr = 0.025   if epoch == 3\n",
      "     |      >>> # lr = 0.05    if epoch >= 4\n",
      "     |      >>> scheduler = ConstantLR(self.opt, factor=0.5, total_iters=4)\n",
      "     |      >>> for epoch in range(100):\n",
      "     |      >>>     train(...)\n",
      "     |      >>>     validate(...)\n",
      "     |      >>>     scheduler.step()\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ConstantLR\n",
      "     |      LRScheduler\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, optimizer, factor=0.3333333333333333, total_iters=5, last_epoch=-1, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_lr(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LRScheduler:\n",
      "     |  \n",
      "     |  get_last_lr(self)\n",
      "     |      Return last computed learning rate by current scheduler.\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict)\n",
      "     |      Loads the schedulers state.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): scheduler state. Should be an object returned\n",
      "     |              from a call to :meth:`state_dict`.\n",
      "     |  \n",
      "     |  print_lr(self, is_verbose, group, lr, epoch=None)\n",
      "     |      Display the current learning rate.\n",
      "     |  \n",
      "     |  state_dict(self)\n",
      "     |      Returns the state of the scheduler as a :class:`dict`.\n",
      "     |      \n",
      "     |      It contains an entry for every variable in self.__dict__ which\n",
      "     |      is not the optimizer.\n",
      "     |  \n",
      "     |  step(self, epoch=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LRScheduler:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class CosineAnnealingLR(LRScheduler)\n",
      "     |  CosineAnnealingLR(optimizer, T_max, eta_min=0, last_epoch=-1, verbose=False)\n",
      "     |  \n",
      "     |  Set the learning rate of each parameter group using a cosine annealing\n",
      "     |  schedule, where :math:`\\eta_{max}` is set to the initial lr and\n",
      "     |  :math:`T_{cur}` is the number of epochs since the last restart in SGDR:\n",
      "     |  \n",
      "     |  .. math::\n",
      "     |      \\begin{aligned}\n",
      "     |          \\eta_t & = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\\left(1\n",
      "     |          + \\cos\\left(\\frac{T_{cur}}{T_{max}}\\pi\\right)\\right),\n",
      "     |          & T_{cur} \\neq (2k+1)T_{max}; \\\\\n",
      "     |          \\eta_{t+1} & = \\eta_{t} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\n",
      "     |          \\left(1 - \\cos\\left(\\frac{1}{T_{max}}\\pi\\right)\\right),\n",
      "     |          & T_{cur} = (2k+1)T_{max}.\n",
      "     |      \\end{aligned}\n",
      "     |  \n",
      "     |  When last_epoch=-1, sets initial lr as lr. Notice that because the schedule\n",
      "     |  is defined recursively, the learning rate can be simultaneously modified\n",
      "     |  outside this scheduler by other operators. If the learning rate is set\n",
      "     |  solely by this scheduler, the learning rate at each step becomes:\n",
      "     |  \n",
      "     |  .. math::\n",
      "     |      \\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\\left(1 +\n",
      "     |      \\cos\\left(\\frac{T_{cur}}{T_{max}}\\pi\\right)\\right)\n",
      "     |  \n",
      "     |  It has been proposed in\n",
      "     |  `SGDR: Stochastic Gradient Descent with Warm Restarts`_. Note that this only\n",
      "     |  implements the cosine annealing part of SGDR, and not the restarts.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      optimizer (Optimizer): Wrapped optimizer.\n",
      "     |      T_max (int): Maximum number of iterations.\n",
      "     |      eta_min (float): Minimum learning rate. Default: 0.\n",
      "     |      last_epoch (int): The index of last epoch. Default: -1.\n",
      "     |      verbose (bool): If ``True``, prints a message to stdout for\n",
      "     |          each update. Default: ``False``.\n",
      "     |  \n",
      "     |  .. _SGDR\\: Stochastic Gradient Descent with Warm Restarts:\n",
      "     |      https://arxiv.org/abs/1608.03983\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CosineAnnealingLR\n",
      "     |      LRScheduler\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, optimizer, T_max, eta_min=0, last_epoch=-1, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_lr(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LRScheduler:\n",
      "     |  \n",
      "     |  get_last_lr(self)\n",
      "     |      Return last computed learning rate by current scheduler.\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict)\n",
      "     |      Loads the schedulers state.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): scheduler state. Should be an object returned\n",
      "     |              from a call to :meth:`state_dict`.\n",
      "     |  \n",
      "     |  print_lr(self, is_verbose, group, lr, epoch=None)\n",
      "     |      Display the current learning rate.\n",
      "     |  \n",
      "     |  state_dict(self)\n",
      "     |      Returns the state of the scheduler as a :class:`dict`.\n",
      "     |      \n",
      "     |      It contains an entry for every variable in self.__dict__ which\n",
      "     |      is not the optimizer.\n",
      "     |  \n",
      "     |  step(self, epoch=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LRScheduler:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class CosineAnnealingWarmRestarts(LRScheduler)\n",
      "     |  CosineAnnealingWarmRestarts(optimizer, T_0, T_mult=1, eta_min=0, last_epoch=-1, verbose=False)\n",
      "     |  \n",
      "     |  Set the learning rate of each parameter group using a cosine annealing\n",
      "     |  schedule, where :math:`\\eta_{max}` is set to the initial lr, :math:`T_{cur}`\n",
      "     |  is the number of epochs since the last restart and :math:`T_{i}` is the number\n",
      "     |  of epochs between two warm restarts in SGDR:\n",
      "     |  \n",
      "     |  .. math::\n",
      "     |      \\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\\left(1 +\n",
      "     |      \\cos\\left(\\frac{T_{cur}}{T_{i}}\\pi\\right)\\right)\n",
      "     |  \n",
      "     |  When :math:`T_{cur}=T_{i}`, set :math:`\\eta_t = \\eta_{min}`.\n",
      "     |  When :math:`T_{cur}=0` after restart, set :math:`\\eta_t=\\eta_{max}`.\n",
      "     |  \n",
      "     |  It has been proposed in\n",
      "     |  `SGDR: Stochastic Gradient Descent with Warm Restarts`_.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      optimizer (Optimizer): Wrapped optimizer.\n",
      "     |      T_0 (int): Number of iterations for the first restart.\n",
      "     |      T_mult (int, optional): A factor increases :math:`T_{i}` after a restart. Default: 1.\n",
      "     |      eta_min (float, optional): Minimum learning rate. Default: 0.\n",
      "     |      last_epoch (int, optional): The index of last epoch. Default: -1.\n",
      "     |      verbose (bool): If ``True``, prints a message to stdout for\n",
      "     |          each update. Default: ``False``.\n",
      "     |  \n",
      "     |  .. _SGDR\\: Stochastic Gradient Descent with Warm Restarts:\n",
      "     |      https://arxiv.org/abs/1608.03983\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CosineAnnealingWarmRestarts\n",
      "     |      LRScheduler\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, optimizer, T_0, T_mult=1, eta_min=0, last_epoch=-1, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_lr(self)\n",
      "     |  \n",
      "     |  step(self, epoch=None)\n",
      "     |      Step could be called after every batch update\n",
      "     |      \n",
      "     |      Example:\n",
      "     |          >>> # xdoctest: +SKIP(\"Undefined vars\")\n",
      "     |          >>> scheduler = CosineAnnealingWarmRestarts(optimizer, T_0, T_mult)\n",
      "     |          >>> iters = len(dataloader)\n",
      "     |          >>> for epoch in range(20):\n",
      "     |          >>>     for i, sample in enumerate(dataloader):\n",
      "     |          >>>         inputs, labels = sample['inputs'], sample['labels']\n",
      "     |          >>>         optimizer.zero_grad()\n",
      "     |          >>>         outputs = net(inputs)\n",
      "     |          >>>         loss = criterion(outputs, labels)\n",
      "     |          >>>         loss.backward()\n",
      "     |          >>>         optimizer.step()\n",
      "     |          >>>         scheduler.step(epoch + i / iters)\n",
      "     |      \n",
      "     |      This function can be called in an interleaved way.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |          >>> # xdoctest: +SKIP(\"Undefined vars\")\n",
      "     |          >>> scheduler = CosineAnnealingWarmRestarts(optimizer, T_0, T_mult)\n",
      "     |          >>> for epoch in range(20):\n",
      "     |          >>>     scheduler.step()\n",
      "     |          >>> scheduler.step(26)\n",
      "     |          >>> scheduler.step() # scheduler.step(27), instead of scheduler(20)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LRScheduler:\n",
      "     |  \n",
      "     |  get_last_lr(self)\n",
      "     |      Return last computed learning rate by current scheduler.\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict)\n",
      "     |      Loads the schedulers state.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): scheduler state. Should be an object returned\n",
      "     |              from a call to :meth:`state_dict`.\n",
      "     |  \n",
      "     |  print_lr(self, is_verbose, group, lr, epoch=None)\n",
      "     |      Display the current learning rate.\n",
      "     |  \n",
      "     |  state_dict(self)\n",
      "     |      Returns the state of the scheduler as a :class:`dict`.\n",
      "     |      \n",
      "     |      It contains an entry for every variable in self.__dict__ which\n",
      "     |      is not the optimizer.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LRScheduler:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class CyclicLR(LRScheduler)\n",
      "     |  CyclicLR(optimizer, base_lr, max_lr, step_size_up=2000, step_size_down=None, mode='triangular', gamma=1.0, scale_fn=None, scale_mode='cycle', cycle_momentum=True, base_momentum=0.8, max_momentum=0.9, last_epoch=-1, verbose=False)\n",
      "     |  \n",
      "     |  Sets the learning rate of each parameter group according to\n",
      "     |  cyclical learning rate policy (CLR). The policy cycles the learning\n",
      "     |  rate between two boundaries with a constant frequency, as detailed in\n",
      "     |  the paper `Cyclical Learning Rates for Training Neural Networks`_.\n",
      "     |  The distance between the two boundaries can be scaled on a per-iteration\n",
      "     |  or per-cycle basis.\n",
      "     |  \n",
      "     |  Cyclical learning rate policy changes the learning rate after every batch.\n",
      "     |  `step` should be called after a batch has been used for training.\n",
      "     |  \n",
      "     |  This class has three built-in policies, as put forth in the paper:\n",
      "     |  \n",
      "     |  * \"triangular\": A basic triangular cycle without amplitude scaling.\n",
      "     |  * \"triangular2\": A basic triangular cycle that scales initial amplitude by half each cycle.\n",
      "     |  * \"exp_range\": A cycle that scales initial amplitude by :math:`\\text{gamma}^{\\text{cycle iterations}}`\n",
      "     |    at each cycle iteration.\n",
      "     |  \n",
      "     |  This implementation was adapted from the github repo: `bckenstler/CLR`_\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      optimizer (Optimizer): Wrapped optimizer.\n",
      "     |      base_lr (float or list): Initial learning rate which is the\n",
      "     |          lower boundary in the cycle for each parameter group.\n",
      "     |      max_lr (float or list): Upper learning rate boundaries in the cycle\n",
      "     |          for each parameter group. Functionally,\n",
      "     |          it defines the cycle amplitude (max_lr - base_lr).\n",
      "     |          The lr at any cycle is the sum of base_lr\n",
      "     |          and some scaling of the amplitude; therefore\n",
      "     |          max_lr may not actually be reached depending on\n",
      "     |          scaling function.\n",
      "     |      step_size_up (int): Number of training iterations in the\n",
      "     |          increasing half of a cycle. Default: 2000\n",
      "     |      step_size_down (int): Number of training iterations in the\n",
      "     |          decreasing half of a cycle. If step_size_down is None,\n",
      "     |          it is set to step_size_up. Default: None\n",
      "     |      mode (str): One of {triangular, triangular2, exp_range}.\n",
      "     |          Values correspond to policies detailed above.\n",
      "     |          If scale_fn is not None, this argument is ignored.\n",
      "     |          Default: 'triangular'\n",
      "     |      gamma (float): Constant in 'exp_range' scaling function:\n",
      "     |          gamma**(cycle iterations)\n",
      "     |          Default: 1.0\n",
      "     |      scale_fn (function): Custom scaling policy defined by a single\n",
      "     |          argument lambda function, where\n",
      "     |          0 <= scale_fn(x) <= 1 for all x >= 0.\n",
      "     |          If specified, then 'mode' is ignored.\n",
      "     |          Default: None\n",
      "     |      scale_mode (str): {'cycle', 'iterations'}.\n",
      "     |          Defines whether scale_fn is evaluated on\n",
      "     |          cycle number or cycle iterations (training\n",
      "     |          iterations since start of cycle).\n",
      "     |          Default: 'cycle'\n",
      "     |      cycle_momentum (bool): If ``True``, momentum is cycled inversely\n",
      "     |          to learning rate between 'base_momentum' and 'max_momentum'.\n",
      "     |          Default: True\n",
      "     |      base_momentum (float or list): Lower momentum boundaries in the cycle\n",
      "     |          for each parameter group. Note that momentum is cycled inversely\n",
      "     |          to learning rate; at the peak of a cycle, momentum is\n",
      "     |          'base_momentum' and learning rate is 'max_lr'.\n",
      "     |          Default: 0.8\n",
      "     |      max_momentum (float or list): Upper momentum boundaries in the cycle\n",
      "     |          for each parameter group. Functionally,\n",
      "     |          it defines the cycle amplitude (max_momentum - base_momentum).\n",
      "     |          The momentum at any cycle is the difference of max_momentum\n",
      "     |          and some scaling of the amplitude; therefore\n",
      "     |          base_momentum may not actually be reached depending on\n",
      "     |          scaling function. Note that momentum is cycled inversely\n",
      "     |          to learning rate; at the start of a cycle, momentum is 'max_momentum'\n",
      "     |          and learning rate is 'base_lr'\n",
      "     |          Default: 0.9\n",
      "     |      last_epoch (int): The index of the last batch. This parameter is used when\n",
      "     |          resuming a training job. Since `step()` should be invoked after each\n",
      "     |          batch instead of after each epoch, this number represents the total\n",
      "     |          number of *batches* computed, not the total number of epochs computed.\n",
      "     |          When last_epoch=-1, the schedule is started from the beginning.\n",
      "     |          Default: -1\n",
      "     |      verbose (bool): If ``True``, prints a message to stdout for\n",
      "     |          each update. Default: ``False``.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      >>> # xdoctest: +SKIP\n",
      "     |      >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
      "     |      >>> scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=0.1)\n",
      "     |      >>> data_loader = torch.utils.data.DataLoader(...)\n",
      "     |      >>> for epoch in range(10):\n",
      "     |      >>>     for batch in data_loader:\n",
      "     |      >>>         train_batch(...)\n",
      "     |      >>>         scheduler.step()\n",
      "     |  \n",
      "     |  \n",
      "     |  .. _Cyclical Learning Rates for Training Neural Networks: https://arxiv.org/abs/1506.01186\n",
      "     |  .. _bckenstler/CLR: https://github.com/bckenstler/CLR\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CyclicLR\n",
      "     |      LRScheduler\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, optimizer, base_lr, max_lr, step_size_up=2000, step_size_down=None, mode='triangular', gamma=1.0, scale_fn=None, scale_mode='cycle', cycle_momentum=True, base_momentum=0.8, max_momentum=0.9, last_epoch=-1, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_lr(self)\n",
      "     |      Calculates the learning rate at batch index. This function treats\n",
      "     |      `self.last_epoch` as the last batch index.\n",
      "     |      \n",
      "     |      If `self.cycle_momentum` is ``True``, this function has a side effect of\n",
      "     |      updating the optimizer's momentum.\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict)\n",
      "     |      Loads the schedulers state.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): scheduler state. Should be an object returned\n",
      "     |              from a call to :meth:`state_dict`.\n",
      "     |  \n",
      "     |  scale_fn(self, x)\n",
      "     |  \n",
      "     |  state_dict(self)\n",
      "     |      Returns the state of the scheduler as a :class:`dict`.\n",
      "     |      \n",
      "     |      It contains an entry for every variable in self.__dict__ which\n",
      "     |      is not the optimizer.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LRScheduler:\n",
      "     |  \n",
      "     |  get_last_lr(self)\n",
      "     |      Return last computed learning rate by current scheduler.\n",
      "     |  \n",
      "     |  print_lr(self, is_verbose, group, lr, epoch=None)\n",
      "     |      Display the current learning rate.\n",
      "     |  \n",
      "     |  step(self, epoch=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LRScheduler:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class ExponentialLR(LRScheduler)\n",
      "     |  ExponentialLR(optimizer, gamma, last_epoch=-1, verbose=False)\n",
      "     |  \n",
      "     |  Decays the learning rate of each parameter group by gamma every epoch.\n",
      "     |  When last_epoch=-1, sets initial lr as lr.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      optimizer (Optimizer): Wrapped optimizer.\n",
      "     |      gamma (float): Multiplicative factor of learning rate decay.\n",
      "     |      last_epoch (int): The index of last epoch. Default: -1.\n",
      "     |      verbose (bool): If ``True``, prints a message to stdout for\n",
      "     |          each update. Default: ``False``.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ExponentialLR\n",
      "     |      LRScheduler\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, optimizer, gamma, last_epoch=-1, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_lr(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LRScheduler:\n",
      "     |  \n",
      "     |  get_last_lr(self)\n",
      "     |      Return last computed learning rate by current scheduler.\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict)\n",
      "     |      Loads the schedulers state.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): scheduler state. Should be an object returned\n",
      "     |              from a call to :meth:`state_dict`.\n",
      "     |  \n",
      "     |  print_lr(self, is_verbose, group, lr, epoch=None)\n",
      "     |      Display the current learning rate.\n",
      "     |  \n",
      "     |  state_dict(self)\n",
      "     |      Returns the state of the scheduler as a :class:`dict`.\n",
      "     |      \n",
      "     |      It contains an entry for every variable in self.__dict__ which\n",
      "     |      is not the optimizer.\n",
      "     |  \n",
      "     |  step(self, epoch=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LRScheduler:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class LRScheduler(builtins.object)\n",
      "     |  LRScheduler(optimizer, last_epoch=-1, verbose=False)\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, optimizer, last_epoch=-1, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_last_lr(self)\n",
      "     |      Return last computed learning rate by current scheduler.\n",
      "     |  \n",
      "     |  get_lr(self)\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict)\n",
      "     |      Loads the schedulers state.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): scheduler state. Should be an object returned\n",
      "     |              from a call to :meth:`state_dict`.\n",
      "     |  \n",
      "     |  print_lr(self, is_verbose, group, lr, epoch=None)\n",
      "     |      Display the current learning rate.\n",
      "     |  \n",
      "     |  state_dict(self)\n",
      "     |      Returns the state of the scheduler as a :class:`dict`.\n",
      "     |      \n",
      "     |      It contains an entry for every variable in self.__dict__ which\n",
      "     |      is not the optimizer.\n",
      "     |  \n",
      "     |  step(self, epoch=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class LambdaLR(LRScheduler)\n",
      "     |  LambdaLR(optimizer, lr_lambda, last_epoch=-1, verbose=False)\n",
      "     |  \n",
      "     |  Sets the learning rate of each parameter group to the initial lr\n",
      "     |  times a given function. When last_epoch=-1, sets initial lr as lr.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      optimizer (Optimizer): Wrapped optimizer.\n",
      "     |      lr_lambda (function or list): A function which computes a multiplicative\n",
      "     |          factor given an integer parameter epoch, or a list of such\n",
      "     |          functions, one for each group in optimizer.param_groups.\n",
      "     |      last_epoch (int): The index of last epoch. Default: -1.\n",
      "     |      verbose (bool): If ``True``, prints a message to stdout for\n",
      "     |          each update. Default: ``False``.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      >>> # xdoctest: +SKIP\n",
      "     |      >>> # Assuming optimizer has two groups.\n",
      "     |      >>> lambda1 = lambda epoch: epoch // 30\n",
      "     |      >>> lambda2 = lambda epoch: 0.95 ** epoch\n",
      "     |      >>> scheduler = LambdaLR(optimizer, lr_lambda=[lambda1, lambda2])\n",
      "     |      >>> for epoch in range(100):\n",
      "     |      >>>     train(...)\n",
      "     |      >>>     validate(...)\n",
      "     |      >>>     scheduler.step()\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LambdaLR\n",
      "     |      LRScheduler\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, optimizer, lr_lambda, last_epoch=-1, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_lr(self)\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict)\n",
      "     |      Loads the schedulers state.\n",
      "     |      \n",
      "     |      When saving or loading the scheduler, please make sure to also save or load the state of the optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): scheduler state. Should be an object returned\n",
      "     |              from a call to :meth:`state_dict`.\n",
      "     |  \n",
      "     |  state_dict(self)\n",
      "     |      Returns the state of the scheduler as a :class:`dict`.\n",
      "     |      \n",
      "     |      It contains an entry for every variable in self.__dict__ which\n",
      "     |      is not the optimizer.\n",
      "     |      The learning rate lambda functions will only be saved if they are callable objects\n",
      "     |      and not if they are functions or lambdas.\n",
      "     |      \n",
      "     |      When saving or loading the scheduler, please make sure to also save or load the state of the optimizer.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LRScheduler:\n",
      "     |  \n",
      "     |  get_last_lr(self)\n",
      "     |      Return last computed learning rate by current scheduler.\n",
      "     |  \n",
      "     |  print_lr(self, is_verbose, group, lr, epoch=None)\n",
      "     |      Display the current learning rate.\n",
      "     |  \n",
      "     |  step(self, epoch=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LRScheduler:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class LinearLR(LRScheduler)\n",
      "     |  LinearLR(optimizer, start_factor=0.3333333333333333, end_factor=1.0, total_iters=5, last_epoch=-1, verbose=False)\n",
      "     |  \n",
      "     |  Decays the learning rate of each parameter group by linearly changing small\n",
      "     |  multiplicative factor until the number of epoch reaches a pre-defined milestone: total_iters.\n",
      "     |  Notice that such decay can happen simultaneously with other changes to the learning rate\n",
      "     |  from outside this scheduler. When last_epoch=-1, sets initial lr as lr.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      optimizer (Optimizer): Wrapped optimizer.\n",
      "     |      start_factor (float): The number we multiply learning rate in the first epoch.\n",
      "     |          The multiplication factor changes towards end_factor in the following epochs.\n",
      "     |          Default: 1./3.\n",
      "     |      end_factor (float): The number we multiply learning rate at the end of linear changing\n",
      "     |          process. Default: 1.0.\n",
      "     |      total_iters (int): The number of iterations that multiplicative factor reaches to 1.\n",
      "     |          Default: 5.\n",
      "     |      last_epoch (int): The index of the last epoch. Default: -1.\n",
      "     |      verbose (bool): If ``True``, prints a message to stdout for\n",
      "     |          each update. Default: ``False``.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      >>> # xdoctest: +SKIP\n",
      "     |      >>> # Assuming optimizer uses lr = 0.05 for all groups\n",
      "     |      >>> # lr = 0.025    if epoch == 0\n",
      "     |      >>> # lr = 0.03125  if epoch == 1\n",
      "     |      >>> # lr = 0.0375   if epoch == 2\n",
      "     |      >>> # lr = 0.04375  if epoch == 3\n",
      "     |      >>> # lr = 0.05    if epoch >= 4\n",
      "     |      >>> scheduler = LinearLR(self.opt, start_factor=0.5, total_iters=4)\n",
      "     |      >>> for epoch in range(100):\n",
      "     |      >>>     train(...)\n",
      "     |      >>>     validate(...)\n",
      "     |      >>>     scheduler.step()\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LinearLR\n",
      "     |      LRScheduler\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, optimizer, start_factor=0.3333333333333333, end_factor=1.0, total_iters=5, last_epoch=-1, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_lr(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LRScheduler:\n",
      "     |  \n",
      "     |  get_last_lr(self)\n",
      "     |      Return last computed learning rate by current scheduler.\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict)\n",
      "     |      Loads the schedulers state.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): scheduler state. Should be an object returned\n",
      "     |              from a call to :meth:`state_dict`.\n",
      "     |  \n",
      "     |  print_lr(self, is_verbose, group, lr, epoch=None)\n",
      "     |      Display the current learning rate.\n",
      "     |  \n",
      "     |  state_dict(self)\n",
      "     |      Returns the state of the scheduler as a :class:`dict`.\n",
      "     |      \n",
      "     |      It contains an entry for every variable in self.__dict__ which\n",
      "     |      is not the optimizer.\n",
      "     |  \n",
      "     |  step(self, epoch=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LRScheduler:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class MultiStepLR(LRScheduler)\n",
      "     |  MultiStepLR(optimizer, milestones, gamma=0.1, last_epoch=-1, verbose=False)\n",
      "     |  \n",
      "     |  Decays the learning rate of each parameter group by gamma once the\n",
      "     |  number of epoch reaches one of the milestones. Notice that such decay can\n",
      "     |  happen simultaneously with other changes to the learning rate from outside\n",
      "     |  this scheduler. When last_epoch=-1, sets initial lr as lr.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      optimizer (Optimizer): Wrapped optimizer.\n",
      "     |      milestones (list): List of epoch indices. Must be increasing.\n",
      "     |      gamma (float): Multiplicative factor of learning rate decay.\n",
      "     |          Default: 0.1.\n",
      "     |      last_epoch (int): The index of last epoch. Default: -1.\n",
      "     |      verbose (bool): If ``True``, prints a message to stdout for\n",
      "     |          each update. Default: ``False``.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      >>> # xdoctest: +SKIP\n",
      "     |      >>> # Assuming optimizer uses lr = 0.05 for all groups\n",
      "     |      >>> # lr = 0.05     if epoch < 30\n",
      "     |      >>> # lr = 0.005    if 30 <= epoch < 80\n",
      "     |      >>> # lr = 0.0005   if epoch >= 80\n",
      "     |      >>> scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n",
      "     |      >>> for epoch in range(100):\n",
      "     |      >>>     train(...)\n",
      "     |      >>>     validate(...)\n",
      "     |      >>>     scheduler.step()\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MultiStepLR\n",
      "     |      LRScheduler\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, optimizer, milestones, gamma=0.1, last_epoch=-1, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_lr(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LRScheduler:\n",
      "     |  \n",
      "     |  get_last_lr(self)\n",
      "     |      Return last computed learning rate by current scheduler.\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict)\n",
      "     |      Loads the schedulers state.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): scheduler state. Should be an object returned\n",
      "     |              from a call to :meth:`state_dict`.\n",
      "     |  \n",
      "     |  print_lr(self, is_verbose, group, lr, epoch=None)\n",
      "     |      Display the current learning rate.\n",
      "     |  \n",
      "     |  state_dict(self)\n",
      "     |      Returns the state of the scheduler as a :class:`dict`.\n",
      "     |      \n",
      "     |      It contains an entry for every variable in self.__dict__ which\n",
      "     |      is not the optimizer.\n",
      "     |  \n",
      "     |  step(self, epoch=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LRScheduler:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class MultiplicativeLR(LRScheduler)\n",
      "     |  MultiplicativeLR(optimizer, lr_lambda, last_epoch=-1, verbose=False)\n",
      "     |  \n",
      "     |  Multiply the learning rate of each parameter group by the factor given\n",
      "     |  in the specified function. When last_epoch=-1, sets initial lr as lr.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      optimizer (Optimizer): Wrapped optimizer.\n",
      "     |      lr_lambda (function or list): A function which computes a multiplicative\n",
      "     |          factor given an integer parameter epoch, or a list of such\n",
      "     |          functions, one for each group in optimizer.param_groups.\n",
      "     |      last_epoch (int): The index of last epoch. Default: -1.\n",
      "     |      verbose (bool): If ``True``, prints a message to stdout for\n",
      "     |          each update. Default: ``False``.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      >>> # xdoctest: +SKIP\n",
      "     |      >>> lmbda = lambda epoch: 0.95\n",
      "     |      >>> scheduler = MultiplicativeLR(optimizer, lr_lambda=lmbda)\n",
      "     |      >>> for epoch in range(100):\n",
      "     |      >>>     train(...)\n",
      "     |      >>>     validate(...)\n",
      "     |      >>>     scheduler.step()\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MultiplicativeLR\n",
      "     |      LRScheduler\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, optimizer, lr_lambda, last_epoch=-1, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_lr(self)\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict)\n",
      "     |      Loads the schedulers state.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): scheduler state. Should be an object returned\n",
      "     |              from a call to :meth:`state_dict`.\n",
      "     |  \n",
      "     |  state_dict(self)\n",
      "     |      Returns the state of the scheduler as a :class:`dict`.\n",
      "     |      \n",
      "     |      It contains an entry for every variable in self.__dict__ which\n",
      "     |      is not the optimizer.\n",
      "     |      The learning rate lambda functions will only be saved if they are callable objects\n",
      "     |      and not if they are functions or lambdas.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LRScheduler:\n",
      "     |  \n",
      "     |  get_last_lr(self)\n",
      "     |      Return last computed learning rate by current scheduler.\n",
      "     |  \n",
      "     |  print_lr(self, is_verbose, group, lr, epoch=None)\n",
      "     |      Display the current learning rate.\n",
      "     |  \n",
      "     |  step(self, epoch=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LRScheduler:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class OneCycleLR(LRScheduler)\n",
      "     |  OneCycleLR(optimizer, max_lr, total_steps=None, epochs=None, steps_per_epoch=None, pct_start=0.3, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=25.0, final_div_factor=10000.0, three_phase=False, last_epoch=-1, verbose=False)\n",
      "     |  \n",
      "     |  Sets the learning rate of each parameter group according to the\n",
      "     |  1cycle learning rate policy. The 1cycle policy anneals the learning\n",
      "     |  rate from an initial learning rate to some maximum learning rate and then\n",
      "     |  from that maximum learning rate to some minimum learning rate much lower\n",
      "     |  than the initial learning rate.\n",
      "     |  This policy was initially described in the paper `Super-Convergence:\n",
      "     |  Very Fast Training of Neural Networks Using Large Learning Rates`_.\n",
      "     |  \n",
      "     |  The 1cycle learning rate policy changes the learning rate after every batch.\n",
      "     |  `step` should be called after a batch has been used for training.\n",
      "     |  \n",
      "     |  This scheduler is not chainable.\n",
      "     |  \n",
      "     |  Note also that the total number of steps in the cycle can be determined in one\n",
      "     |  of two ways (listed in order of precedence):\n",
      "     |  \n",
      "     |  #. A value for total_steps is explicitly provided.\n",
      "     |  #. A number of epochs (epochs) and a number of steps per epoch\n",
      "     |     (steps_per_epoch) are provided.\n",
      "     |     In this case, the number of total steps is inferred by\n",
      "     |     total_steps = epochs * steps_per_epoch\n",
      "     |  \n",
      "     |  You must either provide a value for total_steps or provide a value for both\n",
      "     |  epochs and steps_per_epoch.\n",
      "     |  \n",
      "     |  The default behaviour of this scheduler follows the fastai implementation of 1cycle, which\n",
      "     |  claims that \"unpublished work has shown even better results by using only two phases\". To\n",
      "     |  mimic the behaviour of the original paper instead, set ``three_phase=True``.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      optimizer (Optimizer): Wrapped optimizer.\n",
      "     |      max_lr (float or list): Upper learning rate boundaries in the cycle\n",
      "     |          for each parameter group.\n",
      "     |      total_steps (int): The total number of steps in the cycle. Note that\n",
      "     |          if a value is not provided here, then it must be inferred by providing\n",
      "     |          a value for epochs and steps_per_epoch.\n",
      "     |          Default: None\n",
      "     |      epochs (int): The number of epochs to train for. This is used along\n",
      "     |          with steps_per_epoch in order to infer the total number of steps in the cycle\n",
      "     |          if a value for total_steps is not provided.\n",
      "     |          Default: None\n",
      "     |      steps_per_epoch (int): The number of steps per epoch to train for. This is\n",
      "     |          used along with epochs in order to infer the total number of steps in the\n",
      "     |          cycle if a value for total_steps is not provided.\n",
      "     |          Default: None\n",
      "     |      pct_start (float): The percentage of the cycle (in number of steps) spent\n",
      "     |          increasing the learning rate.\n",
      "     |          Default: 0.3\n",
      "     |      anneal_strategy (str): {'cos', 'linear'}\n",
      "     |          Specifies the annealing strategy: \"cos\" for cosine annealing, \"linear\" for\n",
      "     |          linear annealing.\n",
      "     |          Default: 'cos'\n",
      "     |      cycle_momentum (bool): If ``True``, momentum is cycled inversely\n",
      "     |          to learning rate between 'base_momentum' and 'max_momentum'.\n",
      "     |          Default: True\n",
      "     |      base_momentum (float or list): Lower momentum boundaries in the cycle\n",
      "     |          for each parameter group. Note that momentum is cycled inversely\n",
      "     |          to learning rate; at the peak of a cycle, momentum is\n",
      "     |          'base_momentum' and learning rate is 'max_lr'.\n",
      "     |          Default: 0.85\n",
      "     |      max_momentum (float or list): Upper momentum boundaries in the cycle\n",
      "     |          for each parameter group. Functionally,\n",
      "     |          it defines the cycle amplitude (max_momentum - base_momentum).\n",
      "     |          Note that momentum is cycled inversely\n",
      "     |          to learning rate; at the start of a cycle, momentum is 'max_momentum'\n",
      "     |          and learning rate is 'base_lr'\n",
      "     |          Default: 0.95\n",
      "     |      div_factor (float): Determines the initial learning rate via\n",
      "     |          initial_lr = max_lr/div_factor\n",
      "     |          Default: 25\n",
      "     |      final_div_factor (float): Determines the minimum learning rate via\n",
      "     |          min_lr = initial_lr/final_div_factor\n",
      "     |          Default: 1e4\n",
      "     |      three_phase (bool): If ``True``, use a third phase of the schedule to annihilate the\n",
      "     |          learning rate according to 'final_div_factor' instead of modifying the second\n",
      "     |          phase (the first two phases will be symmetrical about the step indicated by\n",
      "     |          'pct_start').\n",
      "     |      last_epoch (int): The index of the last batch. This parameter is used when\n",
      "     |          resuming a training job. Since `step()` should be invoked after each\n",
      "     |          batch instead of after each epoch, this number represents the total\n",
      "     |          number of *batches* computed, not the total number of epochs computed.\n",
      "     |          When last_epoch=-1, the schedule is started from the beginning.\n",
      "     |          Default: -1\n",
      "     |      verbose (bool): If ``True``, prints a message to stdout for\n",
      "     |          each update. Default: ``False``.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      >>> # xdoctest: +SKIP\n",
      "     |      >>> data_loader = torch.utils.data.DataLoader(...)\n",
      "     |      >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
      "     |      >>> scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(data_loader), epochs=10)\n",
      "     |      >>> for epoch in range(10):\n",
      "     |      >>>     for batch in data_loader:\n",
      "     |      >>>         train_batch(...)\n",
      "     |      >>>         scheduler.step()\n",
      "     |  \n",
      "     |  \n",
      "     |  .. _Super-Convergence\\: Very Fast Training of Neural Networks Using Large Learning Rates:\n",
      "     |      https://arxiv.org/abs/1708.07120\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OneCycleLR\n",
      "     |      LRScheduler\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, optimizer, max_lr, total_steps=None, epochs=None, steps_per_epoch=None, pct_start=0.3, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=25.0, final_div_factor=10000.0, three_phase=False, last_epoch=-1, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_lr(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LRScheduler:\n",
      "     |  \n",
      "     |  get_last_lr(self)\n",
      "     |      Return last computed learning rate by current scheduler.\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict)\n",
      "     |      Loads the schedulers state.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): scheduler state. Should be an object returned\n",
      "     |              from a call to :meth:`state_dict`.\n",
      "     |  \n",
      "     |  print_lr(self, is_verbose, group, lr, epoch=None)\n",
      "     |      Display the current learning rate.\n",
      "     |  \n",
      "     |  state_dict(self)\n",
      "     |      Returns the state of the scheduler as a :class:`dict`.\n",
      "     |      \n",
      "     |      It contains an entry for every variable in self.__dict__ which\n",
      "     |      is not the optimizer.\n",
      "     |  \n",
      "     |  step(self, epoch=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LRScheduler:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class PolynomialLR(LRScheduler)\n",
      "     |  PolynomialLR(optimizer, total_iters=5, power=1.0, last_epoch=-1, verbose=False)\n",
      "     |  \n",
      "     |  Decays the learning rate of each parameter group using a polynomial function\n",
      "     |  in the given total_iters. When last_epoch=-1, sets initial lr as lr.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      optimizer (Optimizer): Wrapped optimizer.\n",
      "     |      total_iters (int): The number of steps that the scheduler decays the learning rate. Default: 5.\n",
      "     |      power (int): The power of the polynomial. Default: 1.0.\n",
      "     |      verbose (bool): If ``True``, prints a message to stdout for\n",
      "     |          each update. Default: ``False``.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |      >>> # Assuming optimizer uses lr = 0.001 for all groups\n",
      "     |      >>> # lr = 0.001     if epoch == 0\n",
      "     |      >>> # lr = 0.00075   if epoch == 1\n",
      "     |      >>> # lr = 0.00050   if epoch == 2\n",
      "     |      >>> # lr = 0.00025   if epoch == 3\n",
      "     |      >>> # lr = 0.0       if epoch >= 4\n",
      "     |      >>> scheduler = PolynomialLR(self.opt, total_iters=4, power=1.0)\n",
      "     |      >>> for epoch in range(100):\n",
      "     |      >>>     train(...)\n",
      "     |      >>>     validate(...)\n",
      "     |      >>>     scheduler.step()\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PolynomialLR\n",
      "     |      LRScheduler\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, optimizer, total_iters=5, power=1.0, last_epoch=-1, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_lr(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LRScheduler:\n",
      "     |  \n",
      "     |  get_last_lr(self)\n",
      "     |      Return last computed learning rate by current scheduler.\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict)\n",
      "     |      Loads the schedulers state.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): scheduler state. Should be an object returned\n",
      "     |              from a call to :meth:`state_dict`.\n",
      "     |  \n",
      "     |  print_lr(self, is_verbose, group, lr, epoch=None)\n",
      "     |      Display the current learning rate.\n",
      "     |  \n",
      "     |  state_dict(self)\n",
      "     |      Returns the state of the scheduler as a :class:`dict`.\n",
      "     |      \n",
      "     |      It contains an entry for every variable in self.__dict__ which\n",
      "     |      is not the optimizer.\n",
      "     |  \n",
      "     |  step(self, epoch=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LRScheduler:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class ReduceLROnPlateau(builtins.object)\n",
      "     |  ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
      "     |  \n",
      "     |  Reduce learning rate when a metric has stopped improving.\n",
      "     |  Models often benefit from reducing the learning rate by a factor\n",
      "     |  of 2-10 once learning stagnates. This scheduler reads a metrics\n",
      "     |  quantity and if no improvement is seen for a 'patience' number\n",
      "     |  of epochs, the learning rate is reduced.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      optimizer (Optimizer): Wrapped optimizer.\n",
      "     |      mode (str): One of `min`, `max`. In `min` mode, lr will\n",
      "     |          be reduced when the quantity monitored has stopped\n",
      "     |          decreasing; in `max` mode it will be reduced when the\n",
      "     |          quantity monitored has stopped increasing. Default: 'min'.\n",
      "     |      factor (float): Factor by which the learning rate will be\n",
      "     |          reduced. new_lr = lr * factor. Default: 0.1.\n",
      "     |      patience (int): Number of epochs with no improvement after\n",
      "     |          which learning rate will be reduced. For example, if\n",
      "     |          `patience = 2`, then we will ignore the first 2 epochs\n",
      "     |          with no improvement, and will only decrease the LR after the\n",
      "     |          3rd epoch if the loss still hasn't improved then.\n",
      "     |          Default: 10.\n",
      "     |      threshold (float): Threshold for measuring the new optimum,\n",
      "     |          to only focus on significant changes. Default: 1e-4.\n",
      "     |      threshold_mode (str): One of `rel`, `abs`. In `rel` mode,\n",
      "     |          dynamic_threshold = best * ( 1 + threshold ) in 'max'\n",
      "     |          mode or best * ( 1 - threshold ) in `min` mode.\n",
      "     |          In `abs` mode, dynamic_threshold = best + threshold in\n",
      "     |          `max` mode or best - threshold in `min` mode. Default: 'rel'.\n",
      "     |      cooldown (int): Number of epochs to wait before resuming\n",
      "     |          normal operation after lr has been reduced. Default: 0.\n",
      "     |      min_lr (float or list): A scalar or a list of scalars. A\n",
      "     |          lower bound on the learning rate of all param groups\n",
      "     |          or each group respectively. Default: 0.\n",
      "     |      eps (float): Minimal decay applied to lr. If the difference\n",
      "     |          between new and old lr is smaller than eps, the update is\n",
      "     |          ignored. Default: 1e-8.\n",
      "     |      verbose (bool): If ``True``, prints a message to stdout for\n",
      "     |          each update. Default: ``False``.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      >>> # xdoctest: +SKIP\n",
      "     |      >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
      "     |      >>> scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
      "     |      >>> for epoch in range(10):\n",
      "     |      >>>     train(...)\n",
      "     |      >>>     val_loss = validate(...)\n",
      "     |      >>>     # Note that step should be called after validate()\n",
      "     |      >>>     scheduler.step(val_loss)\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, optimizer, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  is_better(self, a, best)\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict)\n",
      "     |  \n",
      "     |  state_dict(self)\n",
      "     |  \n",
      "     |  step(self, metrics, epoch=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  in_cooldown\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class SequentialLR(LRScheduler)\n",
      "     |  SequentialLR(optimizer, schedulers, milestones, last_epoch=-1, verbose=False)\n",
      "     |  \n",
      "     |  Receives the list of schedulers that is expected to be called sequentially during\n",
      "     |  optimization process and milestone points that provides exact intervals to reflect\n",
      "     |  which scheduler is supposed to be called at a given epoch.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      optimizer (Optimizer): Wrapped optimizer.\n",
      "     |      schedulers (list): List of chained schedulers.\n",
      "     |      milestones (list): List of integers that reflects milestone points.\n",
      "     |      last_epoch (int): The index of last epoch. Default: -1.\n",
      "     |      verbose (bool): Does nothing.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      >>> # xdoctest: +SKIP\n",
      "     |      >>> # Assuming optimizer uses lr = 1. for all groups\n",
      "     |      >>> # lr = 0.1     if epoch == 0\n",
      "     |      >>> # lr = 0.1     if epoch == 1\n",
      "     |      >>> # lr = 0.9     if epoch == 2\n",
      "     |      >>> # lr = 0.81    if epoch == 3\n",
      "     |      >>> # lr = 0.729   if epoch == 4\n",
      "     |      >>> scheduler1 = ConstantLR(self.opt, factor=0.1, total_iters=2)\n",
      "     |      >>> scheduler2 = ExponentialLR(self.opt, gamma=0.9)\n",
      "     |      >>> scheduler = SequentialLR(self.opt, schedulers=[scheduler1, scheduler2], milestones=[2])\n",
      "     |      >>> for epoch in range(100):\n",
      "     |      >>>     train(...)\n",
      "     |      >>>     validate(...)\n",
      "     |      >>>     scheduler.step()\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SequentialLR\n",
      "     |      LRScheduler\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, optimizer, schedulers, milestones, last_epoch=-1, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict)\n",
      "     |      Loads the schedulers state.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): scheduler state. Should be an object returned\n",
      "     |              from a call to :meth:`state_dict`.\n",
      "     |  \n",
      "     |  state_dict(self)\n",
      "     |      Returns the state of the scheduler as a :class:`dict`.\n",
      "     |      \n",
      "     |      It contains an entry for every variable in self.__dict__ which\n",
      "     |      is not the optimizer.\n",
      "     |      The wrapped scheduler states will also be saved.\n",
      "     |  \n",
      "     |  step(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LRScheduler:\n",
      "     |  \n",
      "     |  get_last_lr(self)\n",
      "     |      Return last computed learning rate by current scheduler.\n",
      "     |  \n",
      "     |  get_lr(self)\n",
      "     |  \n",
      "     |  print_lr(self, is_verbose, group, lr, epoch=None)\n",
      "     |      Display the current learning rate.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LRScheduler:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class StepLR(LRScheduler)\n",
      "     |  StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1, verbose=False)\n",
      "     |  \n",
      "     |  Decays the learning rate of each parameter group by gamma every\n",
      "     |  step_size epochs. Notice that such decay can happen simultaneously with\n",
      "     |  other changes to the learning rate from outside this scheduler. When\n",
      "     |  last_epoch=-1, sets initial lr as lr.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      optimizer (Optimizer): Wrapped optimizer.\n",
      "     |      step_size (int): Period of learning rate decay.\n",
      "     |      gamma (float): Multiplicative factor of learning rate decay.\n",
      "     |          Default: 0.1.\n",
      "     |      last_epoch (int): The index of last epoch. Default: -1.\n",
      "     |      verbose (bool): If ``True``, prints a message to stdout for\n",
      "     |          each update. Default: ``False``.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      >>> # xdoctest: +SKIP\n",
      "     |      >>> # Assuming optimizer uses lr = 0.05 for all groups\n",
      "     |      >>> # lr = 0.05     if epoch < 30\n",
      "     |      >>> # lr = 0.005    if 30 <= epoch < 60\n",
      "     |      >>> # lr = 0.0005   if 60 <= epoch < 90\n",
      "     |      >>> # ...\n",
      "     |      >>> scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
      "     |      >>> for epoch in range(100):\n",
      "     |      >>>     train(...)\n",
      "     |      >>>     validate(...)\n",
      "     |      >>>     scheduler.step()\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      StepLR\n",
      "     |      LRScheduler\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, optimizer, step_size, gamma=0.1, last_epoch=-1, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_lr(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LRScheduler:\n",
      "     |  \n",
      "     |  get_last_lr(self)\n",
      "     |      Return last computed learning rate by current scheduler.\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict)\n",
      "     |      Loads the schedulers state.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): scheduler state. Should be an object returned\n",
      "     |              from a call to :meth:`state_dict`.\n",
      "     |  \n",
      "     |  print_lr(self, is_verbose, group, lr, epoch=None)\n",
      "     |      Display the current learning rate.\n",
      "     |  \n",
      "     |  state_dict(self)\n",
      "     |      Returns the state of the scheduler as a :class:`dict`.\n",
      "     |      \n",
      "     |      It contains an entry for every variable in self.__dict__ which\n",
      "     |      is not the optimizer.\n",
      "     |  \n",
      "     |  step(self, epoch=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LRScheduler:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "DATA\n",
      "    __all__ = ['LambdaLR', 'MultiplicativeLR', 'StepLR', 'MultiStepLR', 'C...\n",
      "\n",
      "FILE\n",
      "    d:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.optim.lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70346870",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T09:15:04.162720Z",
     "start_time": "2023-10-16T09:15:04.140482Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ChainedScheduler',\n",
       " 'ConstantLR',\n",
       " 'CosineAnnealingLR',\n",
       " 'CosineAnnealingWarmRestarts',\n",
       " 'Counter',\n",
       " 'CyclicLR',\n",
       " 'EPOCH_DEPRECATION_WARNING',\n",
       " 'ExponentialLR',\n",
       " 'LRScheduler',\n",
       " 'LambdaLR',\n",
       " 'LinearLR',\n",
       " 'MultiStepLR',\n",
       " 'MultiplicativeLR',\n",
       " 'OneCycleLR',\n",
       " 'Optimizer',\n",
       " 'PolynomialLR',\n",
       " 'ReduceLROnPlateau',\n",
       " 'SequentialLR',\n",
       " 'StepLR',\n",
       " '_LRScheduler',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_enable_get_lr_call',\n",
       " 'bisect_right',\n",
       " 'inf',\n",
       " 'math',\n",
       " 'types',\n",
       " 'warnings',\n",
       " 'weakref',\n",
       " 'wraps']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(torch.optim.lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e4304d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T09:15:22.903942Z",
     "start_time": "2023-10-16T09:15:22.898868Z"
    }
   },
   "outputs": [],
   "source": [
    "# 自定义学习率\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = args.lr * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa490bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有了adjust_learning_rate函数的定义，在训练的过程就可以调用我们的函数来实现学习率的动态变化\n",
    "def adjust_learning_rate(optimizer,...):\n",
    "    ...\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = args.lr,momentum = 0.9)\n",
    "for epoch in range(10):\n",
    "    train(...)\n",
    "    validate(...)\n",
    "    adjust_learning_rate(optimizer,epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
