{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94517185",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T06:38:14.670415Z",
     "start_time": "2023-10-09T06:38:14.662243Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3213fa8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T06:38:15.235456Z",
     "start_time": "2023-10-09T06:38:15.217956Z"
    }
   },
   "outputs": [],
   "source": [
    "# 设置在文件开头的位置\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICE'] = '0'  # 设置默认的显卡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9b7da20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T06:38:17.178231Z",
     "start_time": "2023-10-09T06:38:17.109851Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'CUDA_VISBLE_DEVICE' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISBLE_DEVICE=0,1 python train.py # 使用0，1两块GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6250ca6",
   "metadata": {},
   "source": [
    "### 常见的并行计算方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b815f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据并行\n",
    "# 单卡训练\n",
    "model = Net()\n",
    "model.cuda() # 模型显示转移到CUDA上\n",
    "\n",
    "for image,label in dataloader:\n",
    "    # 图像和标签显示转移到CUDA上\n",
    "    image = image.cuda() \n",
    "    label = label.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351dc51e",
   "metadata": {},
   "source": [
    "### 多卡训练\n",
    "### pyTorch提供了两种多卡训练的方式，分别为DataParallel和DistributedDataParallel（以下我们分别简称为DP和DDP）。\n",
    "### 这两种方法中官方更推荐我们使用DDP，因为它的性能更好。但是DDP的使用比较复杂，而DP经需要改变几行代码既可以实现，所以我们这里先介绍DP，再介绍DDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c522bc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单机多卡DP\n",
    "# 数据并行 (Data parallelism) 的策略：计算任务划分成多个子任务并在多个GPU卡上同时执行这些子任务\n",
    "# nn.DataParallel\n",
    "model = Net()\n",
    "model.cuda() # 模型显示转移到CUDA上\n",
    "\n",
    "if torch.cuda.device_count() > 1: # 含有多张GPU的卡\n",
    "\tmodel = nn.DataParallel(model) # 单机多卡DP训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdf869b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 除此之外，我们也可以指定GPU进行并行训练，一般有两种方式\n",
    "# nn.DataParallel函数传入device_ids参数，可以指定了使用的GPU编号\n",
    "model = nn.DataParallel(model, device_ids=[0,1]) # 使用第0和第1张卡进行并行训练\n",
    "\n",
    "# 注意：要手动指定对程序可见的GPU设备\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a191e301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多级多卡DDP\n",
    "# torch.nn.parallel.DistributedDataParallel（DDP）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a52912",
   "metadata": {},
   "source": [
    "### 不过通过DP进行分布式多卡训练的方式容易造成负载不均衡，有可能第一块GPU显存占用更多，因为输出默认都会被gather到第一块GPU上。为此Pytorch也提供了torch.nn.parallel.DistributedDataParallel（DDP）方法来解决这个问题。\n",
    "\n",
    "### 针对每个GPU，启动一个进程，然后这些进程在最开始的时候会保持一致（模型的初始化参数也一致，每个进程拥有自己的优化器），同时在更新模型的时候，梯度传播也是完全一致的，这样就可以保证任何一个GPU上面的模型参数就是完全一致的，所以这样就不会出现DataParallel那样显存不均衡的问题。不过相对应的，会比较麻烦，接下来介绍一下多机多卡DDP的使用方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d7f25c",
   "metadata": {},
   "source": [
    "### 进程组的相关概念\n",
    "\n",
    "### GROUP：进程组，默认情况下，只有一个组，一个 job 即为一个组，也即一个 world。（当需要进行更加精细的通信时，可以通过 new_group 接口，使用 world 的子集，创建新组，用于集体通信等。）\n",
    "### WORLD_SIZE：表示全局进程个数。如果是多机多卡就表示机器数量，如果是单机多卡就表示 GPU 数量。\n",
    "### RANK：表示进程序号，用于进程间通讯，表征进程优先级。rank = 0 的主机为 master 节点。 如果是多机多卡就表示对应第几台机器，如果是单机多卡，由于一个进程内就只有一个 GPU，所以 rank 也就表示第几块 GPU。\n",
    "### LOCAL_RANK：表示进程内，GPU 编号，非显式参数，由 torch.distributed.launch 内部指定。例如，多机多卡中 rank = 3，local_rank = 0 表示第 3 个进程内的第 1 块 GPU。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17ca626",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DDP的基本用法 (代码编写流程)\n",
    "\n",
    "# 在使用 distributed 包的任何其他函数之前，需要使用 init_process_group 初始化进程组，同时初始化 distributed 包。\n",
    "# 使用 torch.nn.parallel.DistributedDataParallel 创建 分布式模型 DDP(model, device_ids=device_ids)\n",
    "# 使用 torch.utils.data.distributed.DistributedSampler 创建 DataLoader\n",
    "# 使用启动工具 torch.distributed.launch 在每个主机上执行一次脚本，开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc67141f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先是对代码进行修改，添加参数 --local_rank\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--local_rank\", type=int) # 这个参数很重要\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88cf9b5",
   "metadata": {},
   "source": [
    "### 这里的local_rank参数，可以理解为torch.distributed.launch在给一个GPU创建进程的时候，给这个进程提供的GPU号，这个是程序自动给的，不需要手动在命令行中指定这个参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3799391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_rank = int(os.environ[\"LOCAL_RANK\"]) #也可以自动获取"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69820ec",
   "metadata": {},
   "source": [
    "### 然后在所有和GPU相关代码的前面添加如下代码，如果不写这句代码，所有的进程都默认在你使用CUDA_VISIBLE_DEVICES参数设定的0号GPU上面启动"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234a0559",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(args.local_rank) # 调整计算的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df9f5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 接下来我们得初始化backend，也就是俗称的后端，pytorch介绍了以下后端：gloo，nccl，mpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d93923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 当后端选择好了之后, 我们需要设置一下网络接口, 因为多个主机之间肯定是使用网络进行交换, 那肯定就涉及到IP之类的, \n",
    "# 对于nccl和gloo一般会自己寻找网络接口，不过有时候如果网卡比较多的时候，就需要自己设置，可以利用以下代码：\n",
    "import os\n",
    "# 以下二选一, 第一个是使用gloo后端需要设置的, 第二个是使用nccl需要设置的\n",
    "os.environ['GLOO_SOCKET_IFNAME'] = 'eth0'\n",
    "os.environ['NCCL_SOCKET_IFNAME'] = 'eth0'\n",
    "# 注：可以通过以下操作知道自己的网络接口，输入ifconfig, 然后找到自己IP地址的就是, 一般就是em0, eth0, esp2s0之类的,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05930c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 当使用GPU的时候, nccl的效率是高于gloo的，我们一般还是会选择nccl后端，设置GPU之间通信使用的后端和端口：\n",
    "# ps 检查nccl是否可用\n",
    "# torch.distributed.is_nccl_available ()\n",
    "torch.distributed.init_process_group(backend='nccl') # 选择nccl后端，初始化进程组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d699a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 之后，使用 DistributedSampler 对数据集进行划分。\n",
    "# 它能帮助我们将每个 batch 划分成几个 partition，在当前进程中只需要获取和 rank 对应的那个 partition 进行训练：\n",
    "# 创建Dataloader\n",
    "train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, sampler=train_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f63815c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意： testset不用sampler\n",
    "\n",
    "# 然后使用torch.nn.parallel.DistributedDataParallel包装模型：\n",
    "# DDP进行训练\n",
    "model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34fefcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如何启动DDP\n",
    "\n",
    "# 那么如何启动DDP，这不同于DP的方式，需要使用torch.distributed.launch启动器，对于单机多卡的情况：\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=0,1,2,3 python -m torch.distributed.launch --nproc_per_node=4 main.py\n",
    "# nproc_per_node: 这个参数是指你使用这台服务器上面的几张显卡"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
