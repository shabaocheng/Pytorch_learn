{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cb2b1e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T07:44:38.133642Z",
     "start_time": "2023-10-11T07:44:38.116027Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9c59198",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T08:30:21.059989Z",
     "start_time": "2023-10-11T08:30:21.046687Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package torch.optim in torch:\n",
      "\n",
      "NAME\n",
      "    torch.optim\n",
      "\n",
      "DESCRIPTION\n",
      "    :mod:`torch.optim` is a package implementing various optimization algorithms.\n",
      "    Most commonly used methods are already supported, and the interface is general\n",
      "    enough, so that more sophisticated ones can also be easily integrated in the\n",
      "    future.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _functional\n",
      "    _multi_tensor (package)\n",
      "    adadelta\n",
      "    adagrad\n",
      "    adam\n",
      "    adamax\n",
      "    adamw\n",
      "    asgd\n",
      "    lbfgs\n",
      "    lr_scheduler\n",
      "    nadam\n",
      "    optimizer\n",
      "    radam\n",
      "    rmsprop\n",
      "    rprop\n",
      "    sgd\n",
      "    sparse_adam\n",
      "    swa_utils\n",
      "\n",
      "FILE\n",
      "    d:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fb97729",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T08:36:15.735695Z",
     "start_time": "2023-10-11T08:36:15.727102Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_cuda_graph_capture_health_check',\n",
       " '_optimizer_step_code',\n",
       " '_patch_step_function',\n",
       " 'add_param_group',\n",
       " 'load_state_dict',\n",
       " 'profile_hook_step',\n",
       " 'register_step_post_hook',\n",
       " 'register_step_pre_hook',\n",
       " 'state_dict',\n",
       " 'step',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(torch.optim.Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "074a96b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T08:39:51.342692Z",
     "start_time": "2023-10-11T08:39:51.326830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data of weight before step:\n",
      "tensor([[ 1.2185,  0.3876],\n",
      "        [-0.0394,  0.3345]])\n",
      "The grad of weight before step:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "The data of weight after step:\n",
      "tensor([[ 1.1185,  0.2876],\n",
      "        [-0.1394,  0.2345]])\n",
      "The grad of weight after step:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "The grad of weight after optimizer.zero_grad():\n",
      "None\n",
      "optimizer.params_group is \n",
      "[{'params': [tensor([[ 1.1185,  0.2876],\n",
      "        [-0.1394,  0.2345]], requires_grad=True)], 'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False}]\n",
      "weight in optimizer:1811203136448\n",
      "weight in weight:1811203136448\n",
      "\n",
      "optimizer.param_groups is\n",
      "[{'params': [tensor([[ 1.1185,  0.2876],\n",
      "        [-0.1394,  0.2345]], requires_grad=True)], 'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False}, {'params': [tensor([[ 1.4290, -2.5822, -0.6897],\n",
      "        [-0.9650, -1.5027, -0.8874],\n",
      "        [ 0.1270, -1.0689,  0.4609]], requires_grad=True)], 'lr': 0.0001, 'nesterov': True, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'maximize': False, 'foreach': None, 'differentiable': False}]\n",
      "state_dict before step:\n",
      " {'state': {0: {'momentum_buffer': tensor([[1., 1.],\n",
      "        [1., 1.]])}}, 'param_groups': [{'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'params': [0]}, {'lr': 0.0001, 'nesterov': True, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'maximize': False, 'foreach': None, 'differentiable': False, 'params': [1]}]}\n",
      "state_dict after step:\n",
      " {'state': {0: {'momentum_buffer': tensor([[1., 1.],\n",
      "        [1., 1.]])}}, 'param_groups': [{'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'params': [0]}, {'lr': 0.0001, 'nesterov': True, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'maximize': False, 'foreach': None, 'differentiable': False, 'params': [1]}]}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# 设置权重，服从正态分布  --> 2 x 2\n",
    "weight = torch.randn((2, 2), requires_grad=True)\n",
    "# 设置梯度为全1矩阵  --> 2 x 2\n",
    "weight.grad = torch.ones((2, 2))\n",
    "# 输出现有的weight和data\n",
    "print(\"The data of weight before step:\\n{}\".format(weight.data))\n",
    "print(\"The grad of weight before step:\\n{}\".format(weight.grad))\n",
    "# 实例化优化器\n",
    "optimizer = torch.optim.SGD([weight], lr=0.1, momentum=0.9)\n",
    "# 进行一步操作\n",
    "optimizer.step()\n",
    "# 查看进行一步后的值，梯度\n",
    "print(\"The data of weight after step:\\n{}\".format(weight.data))\n",
    "print(\"The grad of weight after step:\\n{}\".format(weight.grad))\n",
    "# 权重清零\n",
    "optimizer.zero_grad()\n",
    "# 检验权重是否为0\n",
    "print(\"The grad of weight after optimizer.zero_grad():\\n{}\".format(weight.grad))\n",
    "# 输出参数\n",
    "print(\"optimizer.params_group is \\n{}\".format(optimizer.param_groups))\n",
    "# 查看参数位置，optimizer和weight的位置一样，我觉得这里可以参考Python是基于值管理\n",
    "print(\"weight in optimizer:{}\\nweight in weight:{}\\n\".format(id(optimizer.param_groups[0]['params'][0]), id(weight)))\n",
    "# 添加参数：weight2\n",
    "weight2 = torch.randn((3, 3), requires_grad=True)\n",
    "optimizer.add_param_group({\"params\": weight2, 'lr': 0.0001, 'nesterov': True})\n",
    "# 查看现有的参数\n",
    "print(\"optimizer.param_groups is\\n{}\".format(optimizer.param_groups))\n",
    "# 查看当前状态信息\n",
    "opt_state_dict = optimizer.state_dict()\n",
    "print(\"state_dict before step:\\n\", opt_state_dict)\n",
    "# 进行5次step操作\n",
    "for _ in range(50):\n",
    "    optimizer.step()\n",
    "# 输出现有状态信息\n",
    "print(\"state_dict after step:\\n\", optimizer.state_dict())\n",
    "# 保存参数信息\n",
    "torch.save(optimizer.state_dict(),os.path.join(r\"D:\\pythonProject\\Attention_Unet\", \"optimizer_state_dict.pkl\"))\n",
    "print(\"----------done-----------\")\n",
    "# 加载参数信息\n",
    "state_dict = torch.load(r\"D:\\pythonProject\\Attention_Unet\\optimizer_state_dict.pkl\") # 需要修改为你自己的路径\n",
    "optimizer.load_state_dict(state_dict)\n",
    "print(\"load state_dict successfully\\n{}\".format(state_dict))\n",
    "# 输出最后属性信息\n",
    "print(\"\\n{}\".format(optimizer.defaults))\n",
    "print(\"\\n{}\".format(optimizer.state))\n",
    "print(\"\\n{}\".format(optimizer.param_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98464066",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T08:39:36.717745Z",
     "start_time": "2023-10-11T08:39:36.704259Z"
    }
   },
   "outputs": [],
   "source": [
    "# 每个优化器都是一个类，我们一定要进行实例化才能使用，比如下方实现：\n",
    "class Net(nn.Module):\n",
    "    pass\n",
    "net = Net()\n",
    "optim = torch.optim.SGD(net.parameters(),lr=0.1)\n",
    "optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd9ae90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer在一个神经网络的epoch中需要实现下面两个步骤：\n",
    "# 梯度置零\n",
    "# 梯度更新\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-5)\n",
    "for epoch in range(EPOCH):\n",
    "    ...\n",
    "    optimizer.zero_grad()  #梯度置零\n",
    "    loss = ...             #计算loss\n",
    "    loss.backward()        #BP反向传播\n",
    "    optimizer.step()       #梯度更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "defd8305",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T08:42:29.232857Z",
     "start_time": "2023-10-11T08:42:27.981794Z"
    }
   },
   "outputs": [],
   "source": [
    "# 给网络不同的层赋予不同的优化器参数。\n",
    "from torch import optim\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "net = resnet18()\n",
    "\n",
    "optimizer = optim.SGD([\n",
    "    {'params':net.fc.parameters()},#fc的lr使用默认的1e-5\n",
    "    {'params':net.layer4[0].conv1.parameters(),'lr':1e-2}],lr=1e-5)\n",
    "\n",
    "# 可以使用param_groups查看属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b126faad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T08:42:31.556765Z",
     "start_time": "2023-10-11T08:42:31.543448Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    differentiable: False\n",
       "    foreach: None\n",
       "    lr: 1e-05\n",
       "    maximize: False\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 1\n",
       "    dampening: 0\n",
       "    differentiable: False\n",
       "    foreach: None\n",
       "    lr: 0.01\n",
       "    maximize: False\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be31a4da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T08:43:15.672791Z",
     "start_time": "2023-10-11T08:43:15.648801Z"
    }
   },
   "outputs": [],
   "source": [
    "# 实验测试\n",
    "a = torch.linspace(-1, 1, 1000)\n",
    "# 升维操作\n",
    "x = torch.unsqueeze(a, dim=1)\n",
    "y = x.pow(2) + 0.1 * torch.normal(torch.zeros(x.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b425eb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T08:43:22.902204Z",
     "start_time": "2023-10-11T08:43:22.889370Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = nn.Linear(1, 20)\n",
    "        self.predict = nn.Linear(20, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.predict(x)\n",
    "        return x\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
