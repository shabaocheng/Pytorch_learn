{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47b250c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-08T02:30:21.013539Z",
     "start_time": "2023-10-08T02:30:21.006800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48c076b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-08T02:30:21.667249Z",
     "start_time": "2023-10-08T02:30:21.655247Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8491, 0.9542, 0.7307],\n",
       "        [0.0132, 0.1874, 0.6667],\n",
       "        [0.6194, 0.4360, 0.7093],\n",
       "        [0.6519, 0.8312, 0.3493]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 张量tensor\n",
    "\n",
    "# 随机创建tensor,随机初始化矩阵 我们可以通过torch.rand()的方法，构造一个随机初始化的矩阵\n",
    "x = torch.rand(4,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9070c119",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-08T02:33:56.615913Z",
     "start_time": "2023-10-08T02:33:56.608719Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.84905434, 0.95418674, 0.73069185],\n",
       "       [0.01316243, 0.18741351, 0.66673636],\n",
       "       [0.6194005 , 0.43599206, 0.7093258 ],\n",
       "       [0.651894  , 0.83118135, 0.34932065]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = x.data.numpy()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10c4fbc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T08:26:15.251783Z",
     "start_time": "2023-10-07T08:26:15.241836Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 全0矩阵的构建 我们可以通过torch.zeros()构造一个矩阵全为 0，并且通过dtype设置数据类型为 long。\n",
    "# 除此以外，我们还可以通过torch.zero_()和torch.zeros_like()将现有矩阵转换为全0矩阵.\n",
    "x = torch.zeros(4,3,dtype=torch.long)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "438d4295",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T08:26:56.621620Z",
     "start_time": "2023-10-07T08:26:56.612963Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.5000, 3.0000])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 张量的构建 我们可以通过torch.tensor()直接使用数据，构造一个张量\n",
    "x = torch.tensor([5.5,3])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dec4b3a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T08:28:28.247365Z",
     "start_time": "2023-10-07T08:28:28.206175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 基于已经存在的 tensor，创建一个 tensor\n",
    "x = x.new_ones(4, 3, dtype=torch.double) \n",
    "# 创建一个新的全1矩阵tensor，返回的tensor默认具有相同的torch.dtype和torch.device\n",
    "# 也可以像之前的写法 x = torch.ones(4, 3, dtype=torch.double)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dac03918",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T08:29:39.862273Z",
     "start_time": "2023-10-07T08:29:39.826736Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0272, -2.9525, -0.1373],\n",
      "        [ 0.4599, -1.9071,  0.0983],\n",
      "        [ 1.6899, -0.6649,  0.7198],\n",
      "        [-0.1709,  2.3047,  0.4438]])\n",
      "torch.Size([4, 3])\n",
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn_like(x, dtype=torch.float)\n",
    "# 重置数据类型\n",
    "print(x)\n",
    "# 结果会有一样的size\n",
    "# 获取它的维度信息\n",
    "print(x.size())\n",
    "print(x.shape)\n",
    "# 注意：返回的torch.Size其实是一个tuple，⽀持所有tuple的操作。我们可以使用索引操作取得张量的长、宽等数据维度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2a6ba0",
   "metadata": {},
   "source": [
    "# 张量的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd3defd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T08:36:38.968735Z",
     "start_time": "2023-10-07T08:36:38.960379Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2335, -2.8705,  0.2542],\n",
      "        [ 1.1063, -1.1673,  0.9826],\n",
      "        [ 2.4224, -0.2676,  0.7671],\n",
      "        [ 0.7319,  3.1444,  0.6378]])\n",
      "tensor([[-0.2335, -2.8705,  0.2542],\n",
      "        [ 1.1063, -1.1673,  0.9826],\n",
      "        [ 2.4224, -0.2676,  0.7671],\n",
      "        [ 0.7319,  3.1444,  0.6378]])\n",
      "tensor([[-0.2335, -2.8705,  0.2542],\n",
      "        [ 1.1063, -1.1673,  0.9826],\n",
      "        [ 2.4224, -0.2676,  0.7671],\n",
      "        [ 0.7319,  3.1444,  0.6378]])\n"
     ]
    }
   ],
   "source": [
    "# 加法操作\n",
    "# 方式1\n",
    "y = torch.rand(4,3)\n",
    "print(x+y)\n",
    "# 方式2\n",
    "print(torch.add(x, y))\n",
    "# 方式3 in-place，原值修改\n",
    "y.add_(x) \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02450dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 索引操作：(类似于numpy)\n",
    "# 需要注意的是：索引出来的结果与原数据共享内存，修改一个，另一个会跟着修改。如果不想修改，可以考虑使用copy()等方法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f2f8814",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T08:37:18.480298Z",
     "start_time": "2023-10-07T08:37:18.435445Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6284, 0.1500, 0.2395, 0.6172])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(4,3)\n",
    "# 取第二列\n",
    "print(x[:, 1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c59649e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T08:37:41.067960Z",
     "start_time": "2023-10-07T08:37:41.027856Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2042, 1.6284, 1.3963])\n",
      "tensor([1.2042, 1.6284, 1.3963])\n"
     ]
    }
   ],
   "source": [
    "y = x[0,:]\n",
    "y += 1\n",
    "print(y)\n",
    "print(x[0, :]) # 源tensor也被改了了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80bd386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 维度变换 张量的维度变换常见的方法有torch.view()和torch.reshape()，下面我们将介绍第一中方法torch.view()："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a313f62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T08:39:06.077945Z",
     "start_time": "2023-10-07T08:39:06.028577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4,4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8) # -1是指这一维的维数由其他维度决定\n",
    "print(x.size(), y.size(), z.size())\n",
    "# 注: torch.view() 返回的新tensor与源tensor共享内存(其实是同一个tensor),更改其中的一个，另外一个也会跟着改变。(顾名思义，view()仅仅是改变了对这个张量的观察角度)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65c51c1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T08:40:06.541804Z",
     "start_time": "2023-10-07T08:40:06.536419Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9027, -0.1971,  0.8981,  0.8638],\n",
      "        [-0.4482,  1.4368,  1.4174,  0.5153],\n",
      "        [ 0.7122,  1.8810, -0.5002,  2.7333],\n",
      "        [-0.3606, -0.1404,  1.3540,  1.5997]])\n",
      "tensor([ 1.9027, -0.1971,  0.8981,  0.8638, -0.4482,  1.4368,  1.4174,  0.5153,\n",
      "         0.7122,  1.8810, -0.5002,  2.7333, -0.3606, -0.1404,  1.3540,  1.5997])\n"
     ]
    }
   ],
   "source": [
    "x += 1\n",
    "print(x)\n",
    "print(y) # 也加了了1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d426eb",
   "metadata": {},
   "source": [
    "## 上面我们说过torch.view()会改变原始张量，但是很多情况下，我们希望原始张量和变换后的张量互相不影响。为为了使创建的张量和原始张量不共享内存，我们需要使用第二种方法torch.reshape()， 同样可以改变张量的形状，但是此函数并不能保证返回的是其拷贝值，所以官方不推荐使用。推荐的方法是我们先用 clone() 创造一个张量副本然后再使用 torch.view()进行函数维度变换 。\n",
    "## 注：使用 clone() 还有一个好处是会被记录在计算图中，即梯度回传到副本时也会传到源 Tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da7dced0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T08:44:44.209396Z",
     "start_time": "2023-10-07T08:44:44.194854Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'float'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1.3996622562408447"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取值操作,如果我们有一个元素 tensor ，我们可以使用 .item() 来获得这个 value，而不获得其他性质\n",
    "x = torch.randn(1)\n",
    "print(type(x))\n",
    "print(type(x.item()))\n",
    "x.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a75df8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T08:45:37.051219Z",
     "start_time": "2023-10-07T08:45:37.042707Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[2, 3],\n",
      "        [3, 4],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "# 广播机制 当对两个形状不同的 Tensor 按元素运算时，可能会触发广播(broadcasting)机制：先适当复制元素使这两个 Tensor 形状相同后再按元素运算。\n",
    "x = torch.arange(1, 3).view(1, 2)\n",
    "print(x)\n",
    "y = torch.arange(1, 4).view(3, 1)\n",
    "print(y)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff87147",
   "metadata": {},
   "source": [
    "## 由于x和y分别是1行2列和3行1列的矩阵，如果要计算x+y，那么x中第一行的2个元素被广播 (复制)到了第二行和第三行，⽽y中第⼀列的3个元素被广播(复制)到了第二列。如此，就可以对2个3行2列的矩阵按元素相加。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
