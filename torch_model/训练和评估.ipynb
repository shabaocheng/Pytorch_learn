{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f728ce8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T07:04:02.398614Z",
     "start_time": "2023-10-11T07:04:02.389203Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e85cb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T07:08:28.692447Z",
     "start_time": "2023-10-11T07:08:28.675506Z"
    }
   },
   "source": [
    "### 如果是训练状态，那么模型的参数应该支持反向传播的修改；如果是验证/测试状态，则不应该修改模型参数。在PyTorch中，模型的状态设置非常简便，如下的两个操作二选一即可："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c71d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()   # 训练状态\n",
    "model.eval()   # 验证/测试状态"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6798c52c",
   "metadata": {},
   "source": [
    "# 训练过程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a32aaa5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T07:05:39.798232Z",
     "start_time": "2023-10-11T07:05:39.091709Z"
    }
   },
   "source": [
    "#### 我们前面在DataLoader构建完成后介绍了如何从中读取数据，在训练过程中使用类似的操作即可，区别在于此时要用for循环读取DataLoader中的全部数据。\n",
    "#### for data, label in train_loader:\n",
    "#### 之后将数据放到GPU上用于后续计算，此处以.cuda()为例\n",
    "#### data, label = data.cuda(), label.cuda()\n",
    "#### 开始用当前批次数据做训练时，应当先将优化器的梯度置零：\n",
    "#### optimizer.zero_grad()\n",
    "#### 之后将data送入模型中训练：\n",
    "#### output = model(data)\n",
    "#### 根据预先定义的criterion计算损失函数：\n",
    "#### loss = criterion(output, label)\n",
    "#### 将loss反向传播回网络：\n",
    "#### loss.backward()\n",
    "#### 使用优化器更新模型参数：\n",
    "#### optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c40532d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T07:13:00.388771Z",
     "start_time": "2023-10-11T07:13:00.382923Z"
    }
   },
   "outputs": [],
   "source": [
    "# 一个完整的训练过程：\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data,label in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.items()*data.size(0)\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    print(\"epoch:{} \\tTraining Loss:{:.6f}\".format(epoch,train_lossr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3812269f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T07:42:11.829063Z",
     "start_time": "2023-10-11T07:42:11.810267Z"
    }
   },
   "outputs": [],
   "source": [
    "# 验证\n",
    "def val(epoch):       \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, label in val_loader:\n",
    "            data, label = data.cuda(), label.cuda()\n",
    "            output = model(data)\n",
    "            preds = torch.argmax(output, 1)\n",
    "            loss = criterion(output, label)\n",
    "            val_loss += loss.item()*data.size(0)\n",
    "            running_accu += torch.sum(preds == label.data)\n",
    "            val_loss = val_loss/len(val_loader.dataset)\n",
    "            print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, val_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
